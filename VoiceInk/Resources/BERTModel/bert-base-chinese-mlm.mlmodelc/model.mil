program(1.0)
[buildInfo = dict<tensor<string, []>, tensor<string, []>>({{"coremlc-component-MIL", "3510.2.1"}, {"coremlc-version", "3500.32.1"}, {"coremltools-component-torch", "2.10.0"}, {"coremltools-source-dialect", "TorchScript"}, {"coremltools-version", "9.0"}})]
{
    func main<ios17>(tensor<int32, [1, ?]> attention_mask, tensor<int32, [1, ?]> input_ids) [FlexibleShapeInformation = tuple<tuple<tensor<string, []>, dict<tensor<string, []>, tensor<int32, [?]>>>, tuple<tensor<string, []>, dict<tensor<string, []>, list<tensor<int32, [2]>, ?>>>>((("DefaultShapes", {{"attention_mask", [1, 128]}, {"input_ids", [1, 128]}}), ("RangeDims", {{"attention_mask", [[1, 1], [1, 512]]}, {"input_ids", [[1, 1], [1, 512]]}})))] {
            tensor<int32, [1, 512]> bert_embeddings_position_ids = const()[name = tensor<string, []>("bert_embeddings_position_ids"), val = tensor<int32, [1, 512]>([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511]])];
            tensor<int32, [1, ?]> input_1 = sub(x = input_ids, y = input_ids)[name = tensor<string, []>("sub_0")];
            tensor<int32, [2]> var_32_shape = shape(x = input_ids)[name = tensor<string, []>("op_32_shape")];
            tensor<int32, []> gather_0_axis_0 = const()[name = tensor<string, []>("gather_0_axis_0"), val = tensor<int32, []>(0)];
            tensor<int32, []> gather_0_batch_dims_0 = const()[name = tensor<string, []>("gather_0_batch_dims_0"), val = tensor<int32, []>(0)];
            tensor<bool, []> gather_0_validate_indices_0 = const()[name = tensor<string, []>("gather_0_validate_indices_0"), val = tensor<bool, []>(false)];
            tensor<string, []> var_32_shape_to_int16_dtype_0 = const()[name = tensor<string, []>("op_32_shape_to_int16_dtype_0"), val = tensor<string, []>("int16")];
            tensor<uint16, []> gather_0_indices_0_to_uint16 = const()[name = tensor<string, []>("gather_0_indices_0_to_uint16"), val = tensor<uint16, []>(1)];
            tensor<int16, [2]> var_32_shape_to_int16 = cast(dtype = var_32_shape_to_int16_dtype_0, x = var_32_shape)[name = tensor<string, []>("cast_57")];
            tensor<int16, []> gather_0_cast_uint16 = gather(axis = gather_0_axis_0, batch_dims = gather_0_batch_dims_0, indices = gather_0_indices_0_to_uint16, validate_indices = gather_0_validate_indices_0, x = var_32_shape_to_int16)[name = tensor<string, []>("gather_0_cast_uint16")];
            tensor<string, []> gather_0_cast_uint16_to_int32_dtype_0 = const()[name = tensor<string, []>("gather_0_cast_uint16_to_int32_dtype_0"), val = tensor<string, []>("int32")];
            tensor<int32, []> concat_0_values0_0 = const()[name = tensor<string, []>("concat_0_values0_0"), val = tensor<int32, []>(1)];
            tensor<int32, []> concat_0_axis_0 = const()[name = tensor<string, []>("concat_0_axis_0"), val = tensor<int32, []>(0)];
            tensor<bool, []> concat_0_interleave_0 = const()[name = tensor<string, []>("concat_0_interleave_0"), val = tensor<bool, []>(false)];
            tensor<int32, []> gather_0_cast_uint16_to_int32 = cast(dtype = gather_0_cast_uint16_to_int32_dtype_0, x = gather_0_cast_uint16)[name = tensor<string, []>("cast_56")];
            tensor<int32, [2]> concat_0 = concat(axis = concat_0_axis_0, interleave = concat_0_interleave_0, values = (concat_0_values0_0, gather_0_cast_uint16_to_int32))[name = tensor<string, []>("concat_0")];
            tensor<int32, [2]> input_3_begin_0 = const()[name = tensor<string, []>("input_3_begin_0"), val = tensor<int32, [2]>([0, 0])];
            tensor<bool, [2]> input_3_end_mask_0 = const()[name = tensor<string, []>("input_3_end_mask_0"), val = tensor<bool, [2]>([true, false])];
            tensor<int32, [1, ?]> input_3 = slice_by_index(begin = input_3_begin_0, end = concat_0, end_mask = input_3_end_mask_0, x = bert_embeddings_position_ids)[name = tensor<string, []>("input_3")];
            tensor<int32, []> inputs_embeds_axis_0 = const()[name = tensor<string, []>("inputs_embeds_axis_0"), val = tensor<int32, []>(0)];
            tensor<int32, []> inputs_embeds_batch_dims_0 = const()[name = tensor<string, []>("inputs_embeds_batch_dims_0"), val = tensor<int32, []>(0)];
            tensor<bool, []> inputs_embeds_validate_indices_0 = const()[name = tensor<string, []>("inputs_embeds_validate_indices_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [21128, 768]> bert_embeddings_word_embeddings_weight_to_fp16 = const()[name = tensor<string, []>("bert_embeddings_word_embeddings_weight_to_fp16"), val = tensor<fp16, [21128, 768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(64)))];
            tensor<string, []> input_ids_to_uint16_dtype_0 = const()[name = tensor<string, []>("input_ids_to_uint16_dtype_0"), val = tensor<string, []>("uint16")];
            tensor<uint16, [1, ?]> input_ids_to_uint16 = cast(dtype = input_ids_to_uint16_dtype_0, x = input_ids)[name = tensor<string, []>("cast_55")];
            tensor<fp16, [1, ?, 768]> inputs_embeds_cast_fp16_cast_uint16 = gather(axis = inputs_embeds_axis_0, batch_dims = inputs_embeds_batch_dims_0, indices = input_ids_to_uint16, validate_indices = inputs_embeds_validate_indices_0, x = bert_embeddings_word_embeddings_weight_to_fp16)[name = tensor<string, []>("inputs_embeds_cast_fp16_cast_uint16")];
            tensor<int32, []> token_type_embeddings_1_axis_0 = const()[name = tensor<string, []>("token_type_embeddings_1_axis_0"), val = tensor<int32, []>(0)];
            tensor<int32, []> token_type_embeddings_1_batch_dims_0 = const()[name = tensor<string, []>("token_type_embeddings_1_batch_dims_0"), val = tensor<int32, []>(0)];
            tensor<bool, []> token_type_embeddings_1_validate_indices_0 = const()[name = tensor<string, []>("token_type_embeddings_1_validate_indices_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [2, 768]> bert_embeddings_token_type_embeddings_weight_to_fp16 = const()[name = tensor<string, []>("bert_embeddings_token_type_embeddings_weight_to_fp16"), val = tensor<fp16, [2, 768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(32452736)))];
            tensor<string, []> input_1_to_uint16_dtype_0 = const()[name = tensor<string, []>("input_1_to_uint16_dtype_0"), val = tensor<string, []>("uint16")];
            tensor<uint16, [1, ?]> input_1_to_uint16 = cast(dtype = input_1_to_uint16_dtype_0, x = input_1)[name = tensor<string, []>("cast_54")];
            tensor<fp16, [1, ?, 768]> token_type_embeddings_1_cast_fp16_cast_uint16 = gather(axis = token_type_embeddings_1_axis_0, batch_dims = token_type_embeddings_1_batch_dims_0, indices = input_1_to_uint16, validate_indices = token_type_embeddings_1_validate_indices_0, x = bert_embeddings_token_type_embeddings_weight_to_fp16)[name = tensor<string, []>("token_type_embeddings_1_cast_fp16_cast_uint16")];
            tensor<fp16, [1, ?, 768]> embeddings_1_cast_fp16 = add(x = inputs_embeds_cast_fp16_cast_uint16, y = token_type_embeddings_1_cast_fp16_cast_uint16)[name = tensor<string, []>("embeddings_1_cast_fp16")];
            tensor<int32, []> position_embeddings_1_axis_0 = const()[name = tensor<string, []>("position_embeddings_1_axis_0"), val = tensor<int32, []>(0)];
            tensor<int32, []> position_embeddings_1_batch_dims_0 = const()[name = tensor<string, []>("position_embeddings_1_batch_dims_0"), val = tensor<int32, []>(0)];
            tensor<bool, []> position_embeddings_1_validate_indices_0 = const()[name = tensor<string, []>("position_embeddings_1_validate_indices_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [512, 768]> bert_embeddings_position_embeddings_weight_to_fp16 = const()[name = tensor<string, []>("bert_embeddings_position_embeddings_weight_to_fp16"), val = tensor<fp16, [512, 768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(32455872)))];
            tensor<string, []> input_3_to_uint16_dtype_0 = const()[name = tensor<string, []>("input_3_to_uint16_dtype_0"), val = tensor<string, []>("uint16")];
            tensor<uint16, [1, ?]> input_3_to_uint16 = cast(dtype = input_3_to_uint16_dtype_0, x = input_3)[name = tensor<string, []>("cast_53")];
            tensor<fp16, [1, ?, 768]> position_embeddings_1_cast_fp16_cast_uint16 = gather(axis = position_embeddings_1_axis_0, batch_dims = position_embeddings_1_batch_dims_0, indices = input_3_to_uint16, validate_indices = position_embeddings_1_validate_indices_0, x = bert_embeddings_position_embeddings_weight_to_fp16)[name = tensor<string, []>("position_embeddings_1_cast_fp16_cast_uint16")];
            tensor<fp16, [1, ?, 768]> input_5_cast_fp16 = add(x = embeddings_1_cast_fp16, y = position_embeddings_1_cast_fp16_cast_uint16)[name = tensor<string, []>("input_5_cast_fp16")];
            tensor<int32, [1]> input_7_axes_0 = const()[name = tensor<string, []>("input_7_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [768]> bert_embeddings_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("bert_embeddings_LayerNorm_weight_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(33242368)))];
            tensor<fp16, [768]> bert_embeddings_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("bert_embeddings_LayerNorm_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(33243968)))];
            tensor<fp16, []> var_20_to_fp16 = const()[name = tensor<string, []>("op_20_to_fp16"), val = tensor<fp16, []>(0x1p-24)];
            tensor<fp16, [1, ?, 768]> input_7_cast_fp16 = layer_norm(axes = input_7_axes_0, beta = bert_embeddings_LayerNorm_bias_to_fp16, epsilon = var_20_to_fp16, gamma = bert_embeddings_LayerNorm_weight_to_fp16, x = input_5_cast_fp16)[name = tensor<string, []>("input_7_cast_fp16")];
            tensor<int32, [1]> var_53_axes_0 = const()[name = tensor<string, []>("op_53_axes_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, [1, 1, ?]> var_53 = expand_dims(axes = var_53_axes_0, x = attention_mask)[name = tensor<string, []>("op_53")];
            tensor<int32, [1]> var_55_axes_0 = const()[name = tensor<string, []>("op_55_axes_0"), val = tensor<int32, [1]>([2])];
            tensor<int32, [1, 1, 1, ?]> var_55 = expand_dims(axes = var_55_axes_0, x = var_53)[name = tensor<string, []>("op_55")];
            tensor<fp16, []> var_61_to_fp16 = const()[name = tensor<string, []>("op_61_to_fp16"), val = tensor<fp16, []>(0x1p+0)];
            tensor<string, []> var_60_to_fp16_dtype_0 = const()[name = tensor<string, []>("op_60_to_fp16_dtype_0"), val = tensor<string, []>("fp16")];
            tensor<fp16, [1, 1, 1, ?]> var_55_to_fp16 = cast(dtype = var_60_to_fp16_dtype_0, x = var_55)[name = tensor<string, []>("cast_52")];
            tensor<fp16, [1, 1, 1, ?]> var_63_cast_fp16 = sub(x = var_61_to_fp16, y = var_55_to_fp16)[name = tensor<string, []>("op_63_cast_fp16")];
            tensor<fp16, []> var_64_to_fp16 = const()[name = tensor<string, []>("op_64_to_fp16"), val = tensor<fp16, []>(-0x1.388p+13)];
            tensor<fp16, [1, 1, 1, ?]> attention_mask_cast_fp16 = mul(x = var_63_cast_fp16, y = var_64_to_fp16)[name = tensor<string, []>("attention_mask_cast_fp16")];
            tensor<fp16, [768, 768]> bert_encoder_layer_0_attention_self_query_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_0_attention_self_query_weight_to_fp16"), val = tensor<fp16, [768, 768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(33245568)))];
            tensor<fp16, [768]> bert_encoder_layer_0_attention_self_query_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_0_attention_self_query_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(34425280)))];
            tensor<fp16, [1, ?, 768]> linear_0_cast_fp16 = linear(bias = bert_encoder_layer_0_attention_self_query_bias_to_fp16, weight = bert_encoder_layer_0_attention_self_query_weight_to_fp16, x = input_7_cast_fp16)[name = tensor<string, []>("linear_0_cast_fp16")];
            tensor<int32, [4]> concat_1x = const()[name = tensor<string, []>("concat_1x"), val = tensor<int32, [4]>([1, -1, 12, 64])];
            tensor<fp16, [1, ?, 12, 64]> var_117_cast_fp16 = reshape(shape = concat_1x, x = linear_0_cast_fp16)[name = tensor<string, []>("op_117_cast_fp16")];
            tensor<fp16, [768, 768]> bert_encoder_layer_0_attention_self_key_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_0_attention_self_key_weight_to_fp16"), val = tensor<fp16, [768, 768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(34426880)))];
            tensor<fp16, [768]> bert_encoder_layer_0_attention_self_key_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_0_attention_self_key_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(35606592)))];
            tensor<fp16, [1, ?, 768]> linear_1_cast_fp16 = linear(bias = bert_encoder_layer_0_attention_self_key_bias_to_fp16, weight = bert_encoder_layer_0_attention_self_key_weight_to_fp16, x = input_7_cast_fp16)[name = tensor<string, []>("linear_1_cast_fp16")];
            tensor<int32, [4]> concat_2x = const()[name = tensor<string, []>("concat_2x"), val = tensor<int32, [4]>([1, -1, 12, 64])];
            tensor<fp16, [1, ?, 12, 64]> var_123_cast_fp16 = reshape(shape = concat_2x, x = linear_1_cast_fp16)[name = tensor<string, []>("op_123_cast_fp16")];
            tensor<fp16, [768, 768]> bert_encoder_layer_0_attention_self_value_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_0_attention_self_value_weight_to_fp16"), val = tensor<fp16, [768, 768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(35608192)))];
            tensor<fp16, [768]> bert_encoder_layer_0_attention_self_value_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_0_attention_self_value_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(36787904)))];
            tensor<fp16, [1, ?, 768]> linear_2_cast_fp16 = linear(bias = bert_encoder_layer_0_attention_self_value_bias_to_fp16, weight = bert_encoder_layer_0_attention_self_value_weight_to_fp16, x = input_7_cast_fp16)[name = tensor<string, []>("linear_2_cast_fp16")];
            tensor<int32, [4]> concat_3x = const()[name = tensor<string, []>("concat_3x"), val = tensor<int32, [4]>([1, -1, 12, 64])];
            tensor<fp16, [1, ?, 12, 64]> var_129_cast_fp16 = reshape(shape = concat_3x, x = linear_2_cast_fp16)[name = tensor<string, []>("op_129_cast_fp16")];
            tensor<int32, [4]> value_1_perm_0 = const()[name = tensor<string, []>("value_1_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<fp16, []> var_77_to_fp16 = const()[name = tensor<string, []>("op_77_to_fp16"), val = tensor<fp16, []>(0x1p-3)];
            tensor<fp16, [1, ?, 12, 64]> mul_0_cast_fp16 = mul(x = var_117_cast_fp16, y = var_77_to_fp16)[name = tensor<string, []>("mul_0_cast_fp16")];
            tensor<bool, []> matmul_0_transpose_y_0 = const()[name = tensor<string, []>("matmul_0_transpose_y_0"), val = tensor<bool, []>(true)];
            tensor<bool, []> matmul_0_transpose_x_0 = const()[name = tensor<string, []>("matmul_0_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<int32, [4]> transpose_48_perm_0 = const()[name = tensor<string, []>("transpose_48_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<int32, [4]> transpose_49_perm_0 = const()[name = tensor<string, []>("transpose_49_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<fp16, [1, 12, ?, 64]> transpose_49 = transpose(perm = transpose_49_perm_0, x = var_123_cast_fp16)[name = tensor<string, []>("transpose_117")];
            tensor<fp16, [1, 12, ?, 64]> transpose_48 = transpose(perm = transpose_48_perm_0, x = mul_0_cast_fp16)[name = tensor<string, []>("transpose_118")];
            tensor<fp16, [1, 12, ?, ?]> matmul_0_cast_fp16 = matmul(transpose_x = matmul_0_transpose_x_0, transpose_y = matmul_0_transpose_y_0, x = transpose_48, y = transpose_49)[name = tensor<string, []>("matmul_0_cast_fp16")];
            tensor<fp16, [1, 12, ?, ?]> add_0_cast_fp16 = add(x = matmul_0_cast_fp16, y = attention_mask_cast_fp16)[name = tensor<string, []>("add_0_cast_fp16")];
            tensor<int32, []> softmax_0_axis_0 = const()[name = tensor<string, []>("softmax_0_axis_0"), val = tensor<int32, []>(-1)];
            tensor<fp16, [1, 12, ?, ?]> softmax_0_cast_fp16 = softmax(axis = softmax_0_axis_0, x = add_0_cast_fp16)[name = tensor<string, []>("softmax_0_cast_fp16")];
            tensor<bool, []> attn_output_1_transpose_x_0 = const()[name = tensor<string, []>("attn_output_1_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> attn_output_1_transpose_y_0 = const()[name = tensor<string, []>("attn_output_1_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 12, ?, 64]> value_1_cast_fp16 = transpose(perm = value_1_perm_0, x = var_129_cast_fp16)[name = tensor<string, []>("transpose_119")];
            tensor<fp16, [1, 12, ?, 64]> attn_output_1_cast_fp16 = matmul(transpose_x = attn_output_1_transpose_x_0, transpose_y = attn_output_1_transpose_y_0, x = softmax_0_cast_fp16, y = value_1_cast_fp16)[name = tensor<string, []>("attn_output_1_cast_fp16")];
            tensor<int32, [4]> var_132_perm_0 = const()[name = tensor<string, []>("op_132_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [3]> concat_4x = const()[name = tensor<string, []>("concat_4x"), val = tensor<int32, [3]>([1, -1, 768])];
            tensor<fp16, [1, ?, 12, 64]> var_132_cast_fp16 = transpose(perm = var_132_perm_0, x = attn_output_1_cast_fp16)[name = tensor<string, []>("transpose_116")];
            tensor<fp16, [1, ?, 768]> var_135_cast_fp16 = reshape(shape = concat_4x, x = var_132_cast_fp16)[name = tensor<string, []>("op_135_cast_fp16")];
            tensor<fp16, [768, 768]> bert_encoder_layer_0_attention_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_0_attention_output_dense_weight_to_fp16"), val = tensor<fp16, [768, 768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(36789504)))];
            tensor<fp16, [768]> bert_encoder_layer_0_attention_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_0_attention_output_dense_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(37969216)))];
            tensor<fp16, [1, ?, 768]> linear_3_cast_fp16 = linear(bias = bert_encoder_layer_0_attention_output_dense_bias_to_fp16, weight = bert_encoder_layer_0_attention_output_dense_weight_to_fp16, x = var_135_cast_fp16)[name = tensor<string, []>("linear_3_cast_fp16")];
            tensor<fp16, [1, ?, 768]> input_13_cast_fp16 = add(x = linear_3_cast_fp16, y = input_7_cast_fp16)[name = tensor<string, []>("input_13_cast_fp16")];
            tensor<int32, [1]> input_15_axes_0 = const()[name = tensor<string, []>("input_15_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [768]> bert_encoder_layer_0_attention_output_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_0_attention_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(37970816)))];
            tensor<fp16, [768]> bert_encoder_layer_0_attention_output_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_0_attention_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(37972416)))];
            tensor<fp16, []> var_68_to_fp16 = const()[name = tensor<string, []>("op_68_to_fp16"), val = tensor<fp16, []>(0x1p-24)];
            tensor<fp16, [1, ?, 768]> input_15_cast_fp16 = layer_norm(axes = input_15_axes_0, beta = bert_encoder_layer_0_attention_output_LayerNorm_bias_to_fp16, epsilon = var_68_to_fp16, gamma = bert_encoder_layer_0_attention_output_LayerNorm_weight_to_fp16, x = input_13_cast_fp16)[name = tensor<string, []>("input_15_cast_fp16")];
            tensor<fp16, [3072, 768]> bert_encoder_layer_0_intermediate_dense_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_0_intermediate_dense_weight_to_fp16"), val = tensor<fp16, [3072, 768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(37974016)))];
            tensor<fp16, [3072]> bert_encoder_layer_0_intermediate_dense_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_0_intermediate_dense_bias_to_fp16"), val = tensor<fp16, [3072]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(42692672)))];
            tensor<fp16, [1, ?, 3072]> linear_4_cast_fp16 = linear(bias = bert_encoder_layer_0_intermediate_dense_bias_to_fp16, weight = bert_encoder_layer_0_intermediate_dense_weight_to_fp16, x = input_15_cast_fp16)[name = tensor<string, []>("linear_4_cast_fp16")];
            tensor<string, []> input_19_mode_0 = const()[name = tensor<string, []>("input_19_mode_0"), val = tensor<string, []>("EXACT")];
            tensor<fp16, [1, ?, 3072]> input_19_cast_fp16 = gelu(mode = input_19_mode_0, x = linear_4_cast_fp16)[name = tensor<string, []>("input_19_cast_fp16")];
            tensor<fp16, [768, 3072]> bert_encoder_layer_0_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_0_output_dense_weight_to_fp16"), val = tensor<fp16, [768, 3072]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(42698880)))];
            tensor<fp16, [768]> bert_encoder_layer_0_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_0_output_dense_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(47417536)))];
            tensor<fp16, [1, ?, 768]> linear_5_cast_fp16 = linear(bias = bert_encoder_layer_0_output_dense_bias_to_fp16, weight = bert_encoder_layer_0_output_dense_weight_to_fp16, x = input_19_cast_fp16)[name = tensor<string, []>("linear_5_cast_fp16")];
            tensor<fp16, [1, ?, 768]> input_23_cast_fp16 = add(x = linear_5_cast_fp16, y = input_15_cast_fp16)[name = tensor<string, []>("input_23_cast_fp16")];
            tensor<int32, [1]> hidden_states_7_axes_0 = const()[name = tensor<string, []>("hidden_states_7_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [768]> bert_encoder_layer_0_output_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_0_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(47419136)))];
            tensor<fp16, [768]> bert_encoder_layer_0_output_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_0_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(47420736)))];
            tensor<fp16, [1, ?, 768]> hidden_states_7_cast_fp16 = layer_norm(axes = hidden_states_7_axes_0, beta = bert_encoder_layer_0_output_LayerNorm_bias_to_fp16, epsilon = var_68_to_fp16, gamma = bert_encoder_layer_0_output_LayerNorm_weight_to_fp16, x = input_23_cast_fp16)[name = tensor<string, []>("hidden_states_7_cast_fp16")];
            tensor<fp16, [768, 768]> bert_encoder_layer_1_attention_self_query_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_1_attention_self_query_weight_to_fp16"), val = tensor<fp16, [768, 768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(47422336)))];
            tensor<fp16, [768]> bert_encoder_layer_1_attention_self_query_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_1_attention_self_query_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(48602048)))];
            tensor<fp16, [1, ?, 768]> linear_6_cast_fp16 = linear(bias = bert_encoder_layer_1_attention_self_query_bias_to_fp16, weight = bert_encoder_layer_1_attention_self_query_weight_to_fp16, x = hidden_states_7_cast_fp16)[name = tensor<string, []>("linear_6_cast_fp16")];
            tensor<int32, [4]> concat_5x = const()[name = tensor<string, []>("concat_5x"), val = tensor<int32, [4]>([1, -1, 12, 64])];
            tensor<fp16, [1, ?, 12, 64]> var_178_cast_fp16 = reshape(shape = concat_5x, x = linear_6_cast_fp16)[name = tensor<string, []>("op_178_cast_fp16")];
            tensor<fp16, [768, 768]> bert_encoder_layer_1_attention_self_key_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_1_attention_self_key_weight_to_fp16"), val = tensor<fp16, [768, 768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(48603648)))];
            tensor<fp16, [768]> bert_encoder_layer_1_attention_self_key_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_1_attention_self_key_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(49783360)))];
            tensor<fp16, [1, ?, 768]> linear_7_cast_fp16 = linear(bias = bert_encoder_layer_1_attention_self_key_bias_to_fp16, weight = bert_encoder_layer_1_attention_self_key_weight_to_fp16, x = hidden_states_7_cast_fp16)[name = tensor<string, []>("linear_7_cast_fp16")];
            tensor<int32, [4]> concat_6x = const()[name = tensor<string, []>("concat_6x"), val = tensor<int32, [4]>([1, -1, 12, 64])];
            tensor<fp16, [1, ?, 12, 64]> var_184_cast_fp16 = reshape(shape = concat_6x, x = linear_7_cast_fp16)[name = tensor<string, []>("op_184_cast_fp16")];
            tensor<fp16, [768, 768]> bert_encoder_layer_1_attention_self_value_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_1_attention_self_value_weight_to_fp16"), val = tensor<fp16, [768, 768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(49784960)))];
            tensor<fp16, [768]> bert_encoder_layer_1_attention_self_value_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_1_attention_self_value_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(50964672)))];
            tensor<fp16, [1, ?, 768]> linear_8_cast_fp16 = linear(bias = bert_encoder_layer_1_attention_self_value_bias_to_fp16, weight = bert_encoder_layer_1_attention_self_value_weight_to_fp16, x = hidden_states_7_cast_fp16)[name = tensor<string, []>("linear_8_cast_fp16")];
            tensor<int32, [4]> concat_7x = const()[name = tensor<string, []>("concat_7x"), val = tensor<int32, [4]>([1, -1, 12, 64])];
            tensor<fp16, [1, ?, 12, 64]> var_190_cast_fp16 = reshape(shape = concat_7x, x = linear_8_cast_fp16)[name = tensor<string, []>("op_190_cast_fp16")];
            tensor<int32, [4]> value_3_perm_0 = const()[name = tensor<string, []>("value_3_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<fp16, [1, ?, 12, 64]> mul_1_cast_fp16 = mul(x = var_178_cast_fp16, y = var_77_to_fp16)[name = tensor<string, []>("mul_1_cast_fp16")];
            tensor<bool, []> matmul_1_transpose_y_0 = const()[name = tensor<string, []>("matmul_1_transpose_y_0"), val = tensor<bool, []>(true)];
            tensor<bool, []> matmul_1_transpose_x_0 = const()[name = tensor<string, []>("matmul_1_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<int32, [4]> transpose_50_perm_0 = const()[name = tensor<string, []>("transpose_50_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<int32, [4]> transpose_51_perm_0 = const()[name = tensor<string, []>("transpose_51_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<fp16, [1, 12, ?, 64]> transpose_51 = transpose(perm = transpose_51_perm_0, x = var_184_cast_fp16)[name = tensor<string, []>("transpose_113")];
            tensor<fp16, [1, 12, ?, 64]> transpose_50 = transpose(perm = transpose_50_perm_0, x = mul_1_cast_fp16)[name = tensor<string, []>("transpose_114")];
            tensor<fp16, [1, 12, ?, ?]> matmul_1_cast_fp16 = matmul(transpose_x = matmul_1_transpose_x_0, transpose_y = matmul_1_transpose_y_0, x = transpose_50, y = transpose_51)[name = tensor<string, []>("matmul_1_cast_fp16")];
            tensor<fp16, [1, 12, ?, ?]> add_1_cast_fp16 = add(x = matmul_1_cast_fp16, y = attention_mask_cast_fp16)[name = tensor<string, []>("add_1_cast_fp16")];
            tensor<int32, []> softmax_1_axis_0 = const()[name = tensor<string, []>("softmax_1_axis_0"), val = tensor<int32, []>(-1)];
            tensor<fp16, [1, 12, ?, ?]> softmax_1_cast_fp16 = softmax(axis = softmax_1_axis_0, x = add_1_cast_fp16)[name = tensor<string, []>("softmax_1_cast_fp16")];
            tensor<bool, []> attn_output_5_transpose_x_0 = const()[name = tensor<string, []>("attn_output_5_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> attn_output_5_transpose_y_0 = const()[name = tensor<string, []>("attn_output_5_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 12, ?, 64]> value_3_cast_fp16 = transpose(perm = value_3_perm_0, x = var_190_cast_fp16)[name = tensor<string, []>("transpose_115")];
            tensor<fp16, [1, 12, ?, 64]> attn_output_5_cast_fp16 = matmul(transpose_x = attn_output_5_transpose_x_0, transpose_y = attn_output_5_transpose_y_0, x = softmax_1_cast_fp16, y = value_3_cast_fp16)[name = tensor<string, []>("attn_output_5_cast_fp16")];
            tensor<int32, [4]> var_193_perm_0 = const()[name = tensor<string, []>("op_193_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [3]> concat_8x = const()[name = tensor<string, []>("concat_8x"), val = tensor<int32, [3]>([1, -1, 768])];
            tensor<fp16, [1, ?, 12, 64]> var_193_cast_fp16 = transpose(perm = var_193_perm_0, x = attn_output_5_cast_fp16)[name = tensor<string, []>("transpose_112")];
            tensor<fp16, [1, ?, 768]> var_196_cast_fp16 = reshape(shape = concat_8x, x = var_193_cast_fp16)[name = tensor<string, []>("op_196_cast_fp16")];
            tensor<fp16, [768, 768]> bert_encoder_layer_1_attention_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_1_attention_output_dense_weight_to_fp16"), val = tensor<fp16, [768, 768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(50966272)))];
            tensor<fp16, [768]> bert_encoder_layer_1_attention_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_1_attention_output_dense_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(52145984)))];
            tensor<fp16, [1, ?, 768]> linear_9_cast_fp16 = linear(bias = bert_encoder_layer_1_attention_output_dense_bias_to_fp16, weight = bert_encoder_layer_1_attention_output_dense_weight_to_fp16, x = var_196_cast_fp16)[name = tensor<string, []>("linear_9_cast_fp16")];
            tensor<fp16, [1, ?, 768]> input_29_cast_fp16 = add(x = linear_9_cast_fp16, y = hidden_states_7_cast_fp16)[name = tensor<string, []>("input_29_cast_fp16")];
            tensor<int32, [1]> input_31_axes_0 = const()[name = tensor<string, []>("input_31_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [768]> bert_encoder_layer_1_attention_output_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_1_attention_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(52147584)))];
            tensor<fp16, [768]> bert_encoder_layer_1_attention_output_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_1_attention_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(52149184)))];
            tensor<fp16, [1, ?, 768]> input_31_cast_fp16 = layer_norm(axes = input_31_axes_0, beta = bert_encoder_layer_1_attention_output_LayerNorm_bias_to_fp16, epsilon = var_68_to_fp16, gamma = bert_encoder_layer_1_attention_output_LayerNorm_weight_to_fp16, x = input_29_cast_fp16)[name = tensor<string, []>("input_31_cast_fp16")];
            tensor<fp16, [3072, 768]> bert_encoder_layer_1_intermediate_dense_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_1_intermediate_dense_weight_to_fp16"), val = tensor<fp16, [3072, 768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(52150784)))];
            tensor<fp16, [3072]> bert_encoder_layer_1_intermediate_dense_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_1_intermediate_dense_bias_to_fp16"), val = tensor<fp16, [3072]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(56869440)))];
            tensor<fp16, [1, ?, 3072]> linear_10_cast_fp16 = linear(bias = bert_encoder_layer_1_intermediate_dense_bias_to_fp16, weight = bert_encoder_layer_1_intermediate_dense_weight_to_fp16, x = input_31_cast_fp16)[name = tensor<string, []>("linear_10_cast_fp16")];
            tensor<string, []> input_35_mode_0 = const()[name = tensor<string, []>("input_35_mode_0"), val = tensor<string, []>("EXACT")];
            tensor<fp16, [1, ?, 3072]> input_35_cast_fp16 = gelu(mode = input_35_mode_0, x = linear_10_cast_fp16)[name = tensor<string, []>("input_35_cast_fp16")];
            tensor<fp16, [768, 3072]> bert_encoder_layer_1_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_1_output_dense_weight_to_fp16"), val = tensor<fp16, [768, 3072]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(56875648)))];
            tensor<fp16, [768]> bert_encoder_layer_1_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_1_output_dense_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(61594304)))];
            tensor<fp16, [1, ?, 768]> linear_11_cast_fp16 = linear(bias = bert_encoder_layer_1_output_dense_bias_to_fp16, weight = bert_encoder_layer_1_output_dense_weight_to_fp16, x = input_35_cast_fp16)[name = tensor<string, []>("linear_11_cast_fp16")];
            tensor<fp16, [1, ?, 768]> input_39_cast_fp16 = add(x = linear_11_cast_fp16, y = input_31_cast_fp16)[name = tensor<string, []>("input_39_cast_fp16")];
            tensor<int32, [1]> hidden_states_13_axes_0 = const()[name = tensor<string, []>("hidden_states_13_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [768]> bert_encoder_layer_1_output_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_1_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(61595904)))];
            tensor<fp16, [768]> bert_encoder_layer_1_output_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_1_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(61597504)))];
            tensor<fp16, [1, ?, 768]> hidden_states_13_cast_fp16 = layer_norm(axes = hidden_states_13_axes_0, beta = bert_encoder_layer_1_output_LayerNorm_bias_to_fp16, epsilon = var_68_to_fp16, gamma = bert_encoder_layer_1_output_LayerNorm_weight_to_fp16, x = input_39_cast_fp16)[name = tensor<string, []>("hidden_states_13_cast_fp16")];
            tensor<fp16, [768, 768]> bert_encoder_layer_2_attention_self_query_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_2_attention_self_query_weight_to_fp16"), val = tensor<fp16, [768, 768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(61599104)))];
            tensor<fp16, [768]> bert_encoder_layer_2_attention_self_query_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_2_attention_self_query_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(62778816)))];
            tensor<fp16, [1, ?, 768]> linear_12_cast_fp16 = linear(bias = bert_encoder_layer_2_attention_self_query_bias_to_fp16, weight = bert_encoder_layer_2_attention_self_query_weight_to_fp16, x = hidden_states_13_cast_fp16)[name = tensor<string, []>("linear_12_cast_fp16")];
            tensor<int32, [4]> concat_9x = const()[name = tensor<string, []>("concat_9x"), val = tensor<int32, [4]>([1, -1, 12, 64])];
            tensor<fp16, [1, ?, 12, 64]> var_239_cast_fp16 = reshape(shape = concat_9x, x = linear_12_cast_fp16)[name = tensor<string, []>("op_239_cast_fp16")];
            tensor<fp16, [768, 768]> bert_encoder_layer_2_attention_self_key_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_2_attention_self_key_weight_to_fp16"), val = tensor<fp16, [768, 768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(62780416)))];
            tensor<fp16, [768]> bert_encoder_layer_2_attention_self_key_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_2_attention_self_key_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(63960128)))];
            tensor<fp16, [1, ?, 768]> linear_13_cast_fp16 = linear(bias = bert_encoder_layer_2_attention_self_key_bias_to_fp16, weight = bert_encoder_layer_2_attention_self_key_weight_to_fp16, x = hidden_states_13_cast_fp16)[name = tensor<string, []>("linear_13_cast_fp16")];
            tensor<int32, [4]> concat_10x = const()[name = tensor<string, []>("concat_10x"), val = tensor<int32, [4]>([1, -1, 12, 64])];
            tensor<fp16, [1, ?, 12, 64]> var_245_cast_fp16 = reshape(shape = concat_10x, x = linear_13_cast_fp16)[name = tensor<string, []>("op_245_cast_fp16")];
            tensor<fp16, [768, 768]> bert_encoder_layer_2_attention_self_value_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_2_attention_self_value_weight_to_fp16"), val = tensor<fp16, [768, 768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(63961728)))];
            tensor<fp16, [768]> bert_encoder_layer_2_attention_self_value_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_2_attention_self_value_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(65141440)))];
            tensor<fp16, [1, ?, 768]> linear_14_cast_fp16 = linear(bias = bert_encoder_layer_2_attention_self_value_bias_to_fp16, weight = bert_encoder_layer_2_attention_self_value_weight_to_fp16, x = hidden_states_13_cast_fp16)[name = tensor<string, []>("linear_14_cast_fp16")];
            tensor<int32, [4]> concat_11x = const()[name = tensor<string, []>("concat_11x"), val = tensor<int32, [4]>([1, -1, 12, 64])];
            tensor<fp16, [1, ?, 12, 64]> var_251_cast_fp16 = reshape(shape = concat_11x, x = linear_14_cast_fp16)[name = tensor<string, []>("op_251_cast_fp16")];
            tensor<int32, [4]> value_5_perm_0 = const()[name = tensor<string, []>("value_5_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<fp16, [1, ?, 12, 64]> mul_2_cast_fp16 = mul(x = var_239_cast_fp16, y = var_77_to_fp16)[name = tensor<string, []>("mul_2_cast_fp16")];
            tensor<bool, []> matmul_2_transpose_y_0 = const()[name = tensor<string, []>("matmul_2_transpose_y_0"), val = tensor<bool, []>(true)];
            tensor<bool, []> matmul_2_transpose_x_0 = const()[name = tensor<string, []>("matmul_2_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<int32, [4]> transpose_52_perm_0 = const()[name = tensor<string, []>("transpose_52_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<int32, [4]> transpose_53_perm_0 = const()[name = tensor<string, []>("transpose_53_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<fp16, [1, 12, ?, 64]> transpose_53 = transpose(perm = transpose_53_perm_0, x = var_245_cast_fp16)[name = tensor<string, []>("transpose_109")];
            tensor<fp16, [1, 12, ?, 64]> transpose_52 = transpose(perm = transpose_52_perm_0, x = mul_2_cast_fp16)[name = tensor<string, []>("transpose_110")];
            tensor<fp16, [1, 12, ?, ?]> matmul_2_cast_fp16 = matmul(transpose_x = matmul_2_transpose_x_0, transpose_y = matmul_2_transpose_y_0, x = transpose_52, y = transpose_53)[name = tensor<string, []>("matmul_2_cast_fp16")];
            tensor<fp16, [1, 12, ?, ?]> add_2_cast_fp16 = add(x = matmul_2_cast_fp16, y = attention_mask_cast_fp16)[name = tensor<string, []>("add_2_cast_fp16")];
            tensor<int32, []> softmax_2_axis_0 = const()[name = tensor<string, []>("softmax_2_axis_0"), val = tensor<int32, []>(-1)];
            tensor<fp16, [1, 12, ?, ?]> softmax_2_cast_fp16 = softmax(axis = softmax_2_axis_0, x = add_2_cast_fp16)[name = tensor<string, []>("softmax_2_cast_fp16")];
            tensor<bool, []> attn_output_9_transpose_x_0 = const()[name = tensor<string, []>("attn_output_9_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> attn_output_9_transpose_y_0 = const()[name = tensor<string, []>("attn_output_9_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 12, ?, 64]> value_5_cast_fp16 = transpose(perm = value_5_perm_0, x = var_251_cast_fp16)[name = tensor<string, []>("transpose_111")];
            tensor<fp16, [1, 12, ?, 64]> attn_output_9_cast_fp16 = matmul(transpose_x = attn_output_9_transpose_x_0, transpose_y = attn_output_9_transpose_y_0, x = softmax_2_cast_fp16, y = value_5_cast_fp16)[name = tensor<string, []>("attn_output_9_cast_fp16")];
            tensor<int32, [4]> var_254_perm_0 = const()[name = tensor<string, []>("op_254_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [3]> concat_12x = const()[name = tensor<string, []>("concat_12x"), val = tensor<int32, [3]>([1, -1, 768])];
            tensor<fp16, [1, ?, 12, 64]> var_254_cast_fp16 = transpose(perm = var_254_perm_0, x = attn_output_9_cast_fp16)[name = tensor<string, []>("transpose_108")];
            tensor<fp16, [1, ?, 768]> var_257_cast_fp16 = reshape(shape = concat_12x, x = var_254_cast_fp16)[name = tensor<string, []>("op_257_cast_fp16")];
            tensor<fp16, [768, 768]> bert_encoder_layer_2_attention_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_2_attention_output_dense_weight_to_fp16"), val = tensor<fp16, [768, 768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(65143040)))];
            tensor<fp16, [768]> bert_encoder_layer_2_attention_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_2_attention_output_dense_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(66322752)))];
            tensor<fp16, [1, ?, 768]> linear_15_cast_fp16 = linear(bias = bert_encoder_layer_2_attention_output_dense_bias_to_fp16, weight = bert_encoder_layer_2_attention_output_dense_weight_to_fp16, x = var_257_cast_fp16)[name = tensor<string, []>("linear_15_cast_fp16")];
            tensor<fp16, [1, ?, 768]> input_45_cast_fp16 = add(x = linear_15_cast_fp16, y = hidden_states_13_cast_fp16)[name = tensor<string, []>("input_45_cast_fp16")];
            tensor<int32, [1]> input_47_axes_0 = const()[name = tensor<string, []>("input_47_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [768]> bert_encoder_layer_2_attention_output_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_2_attention_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(66324352)))];
            tensor<fp16, [768]> bert_encoder_layer_2_attention_output_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_2_attention_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(66325952)))];
            tensor<fp16, [1, ?, 768]> input_47_cast_fp16 = layer_norm(axes = input_47_axes_0, beta = bert_encoder_layer_2_attention_output_LayerNorm_bias_to_fp16, epsilon = var_68_to_fp16, gamma = bert_encoder_layer_2_attention_output_LayerNorm_weight_to_fp16, x = input_45_cast_fp16)[name = tensor<string, []>("input_47_cast_fp16")];
            tensor<fp16, [3072, 768]> bert_encoder_layer_2_intermediate_dense_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_2_intermediate_dense_weight_to_fp16"), val = tensor<fp16, [3072, 768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(66327552)))];
            tensor<fp16, [3072]> bert_encoder_layer_2_intermediate_dense_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_2_intermediate_dense_bias_to_fp16"), val = tensor<fp16, [3072]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(71046208)))];
            tensor<fp16, [1, ?, 3072]> linear_16_cast_fp16 = linear(bias = bert_encoder_layer_2_intermediate_dense_bias_to_fp16, weight = bert_encoder_layer_2_intermediate_dense_weight_to_fp16, x = input_47_cast_fp16)[name = tensor<string, []>("linear_16_cast_fp16")];
            tensor<string, []> input_51_mode_0 = const()[name = tensor<string, []>("input_51_mode_0"), val = tensor<string, []>("EXACT")];
            tensor<fp16, [1, ?, 3072]> input_51_cast_fp16 = gelu(mode = input_51_mode_0, x = linear_16_cast_fp16)[name = tensor<string, []>("input_51_cast_fp16")];
            tensor<fp16, [768, 3072]> bert_encoder_layer_2_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_2_output_dense_weight_to_fp16"), val = tensor<fp16, [768, 3072]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(71052416)))];
            tensor<fp16, [768]> bert_encoder_layer_2_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_2_output_dense_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(75771072)))];
            tensor<fp16, [1, ?, 768]> linear_17_cast_fp16 = linear(bias = bert_encoder_layer_2_output_dense_bias_to_fp16, weight = bert_encoder_layer_2_output_dense_weight_to_fp16, x = input_51_cast_fp16)[name = tensor<string, []>("linear_17_cast_fp16")];
            tensor<fp16, [1, ?, 768]> input_55_cast_fp16 = add(x = linear_17_cast_fp16, y = input_47_cast_fp16)[name = tensor<string, []>("input_55_cast_fp16")];
            tensor<int32, [1]> hidden_states_19_axes_0 = const()[name = tensor<string, []>("hidden_states_19_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [768]> bert_encoder_layer_2_output_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_2_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(75772672)))];
            tensor<fp16, [768]> bert_encoder_layer_2_output_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_2_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(75774272)))];
            tensor<fp16, [1, ?, 768]> hidden_states_19_cast_fp16 = layer_norm(axes = hidden_states_19_axes_0, beta = bert_encoder_layer_2_output_LayerNorm_bias_to_fp16, epsilon = var_68_to_fp16, gamma = bert_encoder_layer_2_output_LayerNorm_weight_to_fp16, x = input_55_cast_fp16)[name = tensor<string, []>("hidden_states_19_cast_fp16")];
            tensor<fp16, [768, 768]> bert_encoder_layer_3_attention_self_query_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_3_attention_self_query_weight_to_fp16"), val = tensor<fp16, [768, 768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(75775872)))];
            tensor<fp16, [768]> bert_encoder_layer_3_attention_self_query_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_3_attention_self_query_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(76955584)))];
            tensor<fp16, [1, ?, 768]> linear_18_cast_fp16 = linear(bias = bert_encoder_layer_3_attention_self_query_bias_to_fp16, weight = bert_encoder_layer_3_attention_self_query_weight_to_fp16, x = hidden_states_19_cast_fp16)[name = tensor<string, []>("linear_18_cast_fp16")];
            tensor<int32, [4]> concat_13x = const()[name = tensor<string, []>("concat_13x"), val = tensor<int32, [4]>([1, -1, 12, 64])];
            tensor<fp16, [1, ?, 12, 64]> var_300_cast_fp16 = reshape(shape = concat_13x, x = linear_18_cast_fp16)[name = tensor<string, []>("op_300_cast_fp16")];
            tensor<fp16, [768, 768]> bert_encoder_layer_3_attention_self_key_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_3_attention_self_key_weight_to_fp16"), val = tensor<fp16, [768, 768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(76957184)))];
            tensor<fp16, [768]> bert_encoder_layer_3_attention_self_key_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_3_attention_self_key_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(78136896)))];
            tensor<fp16, [1, ?, 768]> linear_19_cast_fp16 = linear(bias = bert_encoder_layer_3_attention_self_key_bias_to_fp16, weight = bert_encoder_layer_3_attention_self_key_weight_to_fp16, x = hidden_states_19_cast_fp16)[name = tensor<string, []>("linear_19_cast_fp16")];
            tensor<int32, [4]> concat_14x = const()[name = tensor<string, []>("concat_14x"), val = tensor<int32, [4]>([1, -1, 12, 64])];
            tensor<fp16, [1, ?, 12, 64]> var_306_cast_fp16 = reshape(shape = concat_14x, x = linear_19_cast_fp16)[name = tensor<string, []>("op_306_cast_fp16")];
            tensor<fp16, [768, 768]> bert_encoder_layer_3_attention_self_value_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_3_attention_self_value_weight_to_fp16"), val = tensor<fp16, [768, 768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(78138496)))];
            tensor<fp16, [768]> bert_encoder_layer_3_attention_self_value_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_3_attention_self_value_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(79318208)))];
            tensor<fp16, [1, ?, 768]> linear_20_cast_fp16 = linear(bias = bert_encoder_layer_3_attention_self_value_bias_to_fp16, weight = bert_encoder_layer_3_attention_self_value_weight_to_fp16, x = hidden_states_19_cast_fp16)[name = tensor<string, []>("linear_20_cast_fp16")];
            tensor<int32, [4]> concat_15x = const()[name = tensor<string, []>("concat_15x"), val = tensor<int32, [4]>([1, -1, 12, 64])];
            tensor<fp16, [1, ?, 12, 64]> var_312_cast_fp16 = reshape(shape = concat_15x, x = linear_20_cast_fp16)[name = tensor<string, []>("op_312_cast_fp16")];
            tensor<int32, [4]> value_7_perm_0 = const()[name = tensor<string, []>("value_7_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<fp16, [1, ?, 12, 64]> mul_3_cast_fp16 = mul(x = var_300_cast_fp16, y = var_77_to_fp16)[name = tensor<string, []>("mul_3_cast_fp16")];
            tensor<bool, []> matmul_3_transpose_y_0 = const()[name = tensor<string, []>("matmul_3_transpose_y_0"), val = tensor<bool, []>(true)];
            tensor<bool, []> matmul_3_transpose_x_0 = const()[name = tensor<string, []>("matmul_3_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<int32, [4]> transpose_54_perm_0 = const()[name = tensor<string, []>("transpose_54_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<int32, [4]> transpose_55_perm_0 = const()[name = tensor<string, []>("transpose_55_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<fp16, [1, 12, ?, 64]> transpose_55 = transpose(perm = transpose_55_perm_0, x = var_306_cast_fp16)[name = tensor<string, []>("transpose_105")];
            tensor<fp16, [1, 12, ?, 64]> transpose_54 = transpose(perm = transpose_54_perm_0, x = mul_3_cast_fp16)[name = tensor<string, []>("transpose_106")];
            tensor<fp16, [1, 12, ?, ?]> matmul_3_cast_fp16 = matmul(transpose_x = matmul_3_transpose_x_0, transpose_y = matmul_3_transpose_y_0, x = transpose_54, y = transpose_55)[name = tensor<string, []>("matmul_3_cast_fp16")];
            tensor<fp16, [1, 12, ?, ?]> add_3_cast_fp16 = add(x = matmul_3_cast_fp16, y = attention_mask_cast_fp16)[name = tensor<string, []>("add_3_cast_fp16")];
            tensor<int32, []> softmax_3_axis_0 = const()[name = tensor<string, []>("softmax_3_axis_0"), val = tensor<int32, []>(-1)];
            tensor<fp16, [1, 12, ?, ?]> softmax_3_cast_fp16 = softmax(axis = softmax_3_axis_0, x = add_3_cast_fp16)[name = tensor<string, []>("softmax_3_cast_fp16")];
            tensor<bool, []> attn_output_13_transpose_x_0 = const()[name = tensor<string, []>("attn_output_13_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> attn_output_13_transpose_y_0 = const()[name = tensor<string, []>("attn_output_13_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 12, ?, 64]> value_7_cast_fp16 = transpose(perm = value_7_perm_0, x = var_312_cast_fp16)[name = tensor<string, []>("transpose_107")];
            tensor<fp16, [1, 12, ?, 64]> attn_output_13_cast_fp16 = matmul(transpose_x = attn_output_13_transpose_x_0, transpose_y = attn_output_13_transpose_y_0, x = softmax_3_cast_fp16, y = value_7_cast_fp16)[name = tensor<string, []>("attn_output_13_cast_fp16")];
            tensor<int32, [4]> var_315_perm_0 = const()[name = tensor<string, []>("op_315_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [3]> concat_16x = const()[name = tensor<string, []>("concat_16x"), val = tensor<int32, [3]>([1, -1, 768])];
            tensor<fp16, [1, ?, 12, 64]> var_315_cast_fp16 = transpose(perm = var_315_perm_0, x = attn_output_13_cast_fp16)[name = tensor<string, []>("transpose_104")];
            tensor<fp16, [1, ?, 768]> var_318_cast_fp16 = reshape(shape = concat_16x, x = var_315_cast_fp16)[name = tensor<string, []>("op_318_cast_fp16")];
            tensor<fp16, [768, 768]> bert_encoder_layer_3_attention_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_3_attention_output_dense_weight_to_fp16"), val = tensor<fp16, [768, 768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(79319808)))];
            tensor<fp16, [768]> bert_encoder_layer_3_attention_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_3_attention_output_dense_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(80499520)))];
            tensor<fp16, [1, ?, 768]> linear_21_cast_fp16 = linear(bias = bert_encoder_layer_3_attention_output_dense_bias_to_fp16, weight = bert_encoder_layer_3_attention_output_dense_weight_to_fp16, x = var_318_cast_fp16)[name = tensor<string, []>("linear_21_cast_fp16")];
            tensor<fp16, [1, ?, 768]> input_61_cast_fp16 = add(x = linear_21_cast_fp16, y = hidden_states_19_cast_fp16)[name = tensor<string, []>("input_61_cast_fp16")];
            tensor<int32, [1]> input_63_axes_0 = const()[name = tensor<string, []>("input_63_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [768]> bert_encoder_layer_3_attention_output_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_3_attention_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(80501120)))];
            tensor<fp16, [768]> bert_encoder_layer_3_attention_output_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_3_attention_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(80502720)))];
            tensor<fp16, [1, ?, 768]> input_63_cast_fp16 = layer_norm(axes = input_63_axes_0, beta = bert_encoder_layer_3_attention_output_LayerNorm_bias_to_fp16, epsilon = var_68_to_fp16, gamma = bert_encoder_layer_3_attention_output_LayerNorm_weight_to_fp16, x = input_61_cast_fp16)[name = tensor<string, []>("input_63_cast_fp16")];
            tensor<fp16, [3072, 768]> bert_encoder_layer_3_intermediate_dense_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_3_intermediate_dense_weight_to_fp16"), val = tensor<fp16, [3072, 768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(80504320)))];
            tensor<fp16, [3072]> bert_encoder_layer_3_intermediate_dense_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_3_intermediate_dense_bias_to_fp16"), val = tensor<fp16, [3072]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(85222976)))];
            tensor<fp16, [1, ?, 3072]> linear_22_cast_fp16 = linear(bias = bert_encoder_layer_3_intermediate_dense_bias_to_fp16, weight = bert_encoder_layer_3_intermediate_dense_weight_to_fp16, x = input_63_cast_fp16)[name = tensor<string, []>("linear_22_cast_fp16")];
            tensor<string, []> input_67_mode_0 = const()[name = tensor<string, []>("input_67_mode_0"), val = tensor<string, []>("EXACT")];
            tensor<fp16, [1, ?, 3072]> input_67_cast_fp16 = gelu(mode = input_67_mode_0, x = linear_22_cast_fp16)[name = tensor<string, []>("input_67_cast_fp16")];
            tensor<fp16, [768, 3072]> bert_encoder_layer_3_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_3_output_dense_weight_to_fp16"), val = tensor<fp16, [768, 3072]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(85229184)))];
            tensor<fp16, [768]> bert_encoder_layer_3_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_3_output_dense_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(89947840)))];
            tensor<fp16, [1, ?, 768]> linear_23_cast_fp16 = linear(bias = bert_encoder_layer_3_output_dense_bias_to_fp16, weight = bert_encoder_layer_3_output_dense_weight_to_fp16, x = input_67_cast_fp16)[name = tensor<string, []>("linear_23_cast_fp16")];
            tensor<fp16, [1, ?, 768]> input_71_cast_fp16 = add(x = linear_23_cast_fp16, y = input_63_cast_fp16)[name = tensor<string, []>("input_71_cast_fp16")];
            tensor<int32, [1]> hidden_states_25_axes_0 = const()[name = tensor<string, []>("hidden_states_25_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [768]> bert_encoder_layer_3_output_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_3_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(89949440)))];
            tensor<fp16, [768]> bert_encoder_layer_3_output_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_3_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(89951040)))];
            tensor<fp16, [1, ?, 768]> hidden_states_25_cast_fp16 = layer_norm(axes = hidden_states_25_axes_0, beta = bert_encoder_layer_3_output_LayerNorm_bias_to_fp16, epsilon = var_68_to_fp16, gamma = bert_encoder_layer_3_output_LayerNorm_weight_to_fp16, x = input_71_cast_fp16)[name = tensor<string, []>("hidden_states_25_cast_fp16")];
            tensor<fp16, [768, 768]> bert_encoder_layer_4_attention_self_query_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_4_attention_self_query_weight_to_fp16"), val = tensor<fp16, [768, 768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(89952640)))];
            tensor<fp16, [768]> bert_encoder_layer_4_attention_self_query_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_4_attention_self_query_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(91132352)))];
            tensor<fp16, [1, ?, 768]> linear_24_cast_fp16 = linear(bias = bert_encoder_layer_4_attention_self_query_bias_to_fp16, weight = bert_encoder_layer_4_attention_self_query_weight_to_fp16, x = hidden_states_25_cast_fp16)[name = tensor<string, []>("linear_24_cast_fp16")];
            tensor<int32, [4]> concat_17x = const()[name = tensor<string, []>("concat_17x"), val = tensor<int32, [4]>([1, -1, 12, 64])];
            tensor<fp16, [1, ?, 12, 64]> var_361_cast_fp16 = reshape(shape = concat_17x, x = linear_24_cast_fp16)[name = tensor<string, []>("op_361_cast_fp16")];
            tensor<fp16, [768, 768]> bert_encoder_layer_4_attention_self_key_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_4_attention_self_key_weight_to_fp16"), val = tensor<fp16, [768, 768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(91133952)))];
            tensor<fp16, [768]> bert_encoder_layer_4_attention_self_key_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_4_attention_self_key_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(92313664)))];
            tensor<fp16, [1, ?, 768]> linear_25_cast_fp16 = linear(bias = bert_encoder_layer_4_attention_self_key_bias_to_fp16, weight = bert_encoder_layer_4_attention_self_key_weight_to_fp16, x = hidden_states_25_cast_fp16)[name = tensor<string, []>("linear_25_cast_fp16")];
            tensor<int32, [4]> concat_18x = const()[name = tensor<string, []>("concat_18x"), val = tensor<int32, [4]>([1, -1, 12, 64])];
            tensor<fp16, [1, ?, 12, 64]> var_367_cast_fp16 = reshape(shape = concat_18x, x = linear_25_cast_fp16)[name = tensor<string, []>("op_367_cast_fp16")];
            tensor<fp16, [768, 768]> bert_encoder_layer_4_attention_self_value_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_4_attention_self_value_weight_to_fp16"), val = tensor<fp16, [768, 768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(92315264)))];
            tensor<fp16, [768]> bert_encoder_layer_4_attention_self_value_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_4_attention_self_value_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(93494976)))];
            tensor<fp16, [1, ?, 768]> linear_26_cast_fp16 = linear(bias = bert_encoder_layer_4_attention_self_value_bias_to_fp16, weight = bert_encoder_layer_4_attention_self_value_weight_to_fp16, x = hidden_states_25_cast_fp16)[name = tensor<string, []>("linear_26_cast_fp16")];
            tensor<int32, [4]> concat_19x = const()[name = tensor<string, []>("concat_19x"), val = tensor<int32, [4]>([1, -1, 12, 64])];
            tensor<fp16, [1, ?, 12, 64]> var_373_cast_fp16 = reshape(shape = concat_19x, x = linear_26_cast_fp16)[name = tensor<string, []>("op_373_cast_fp16")];
            tensor<int32, [4]> value_9_perm_0 = const()[name = tensor<string, []>("value_9_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<fp16, [1, ?, 12, 64]> mul_4_cast_fp16 = mul(x = var_361_cast_fp16, y = var_77_to_fp16)[name = tensor<string, []>("mul_4_cast_fp16")];
            tensor<bool, []> matmul_4_transpose_y_0 = const()[name = tensor<string, []>("matmul_4_transpose_y_0"), val = tensor<bool, []>(true)];
            tensor<bool, []> matmul_4_transpose_x_0 = const()[name = tensor<string, []>("matmul_4_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<int32, [4]> transpose_56_perm_0 = const()[name = tensor<string, []>("transpose_56_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<int32, [4]> transpose_57_perm_0 = const()[name = tensor<string, []>("transpose_57_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<fp16, [1, 12, ?, 64]> transpose_57 = transpose(perm = transpose_57_perm_0, x = var_367_cast_fp16)[name = tensor<string, []>("transpose_101")];
            tensor<fp16, [1, 12, ?, 64]> transpose_56 = transpose(perm = transpose_56_perm_0, x = mul_4_cast_fp16)[name = tensor<string, []>("transpose_102")];
            tensor<fp16, [1, 12, ?, ?]> matmul_4_cast_fp16 = matmul(transpose_x = matmul_4_transpose_x_0, transpose_y = matmul_4_transpose_y_0, x = transpose_56, y = transpose_57)[name = tensor<string, []>("matmul_4_cast_fp16")];
            tensor<fp16, [1, 12, ?, ?]> add_4_cast_fp16 = add(x = matmul_4_cast_fp16, y = attention_mask_cast_fp16)[name = tensor<string, []>("add_4_cast_fp16")];
            tensor<int32, []> softmax_4_axis_0 = const()[name = tensor<string, []>("softmax_4_axis_0"), val = tensor<int32, []>(-1)];
            tensor<fp16, [1, 12, ?, ?]> softmax_4_cast_fp16 = softmax(axis = softmax_4_axis_0, x = add_4_cast_fp16)[name = tensor<string, []>("softmax_4_cast_fp16")];
            tensor<bool, []> attn_output_17_transpose_x_0 = const()[name = tensor<string, []>("attn_output_17_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> attn_output_17_transpose_y_0 = const()[name = tensor<string, []>("attn_output_17_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 12, ?, 64]> value_9_cast_fp16 = transpose(perm = value_9_perm_0, x = var_373_cast_fp16)[name = tensor<string, []>("transpose_103")];
            tensor<fp16, [1, 12, ?, 64]> attn_output_17_cast_fp16 = matmul(transpose_x = attn_output_17_transpose_x_0, transpose_y = attn_output_17_transpose_y_0, x = softmax_4_cast_fp16, y = value_9_cast_fp16)[name = tensor<string, []>("attn_output_17_cast_fp16")];
            tensor<int32, [4]> var_376_perm_0 = const()[name = tensor<string, []>("op_376_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [3]> concat_20x = const()[name = tensor<string, []>("concat_20x"), val = tensor<int32, [3]>([1, -1, 768])];
            tensor<fp16, [1, ?, 12, 64]> var_376_cast_fp16 = transpose(perm = var_376_perm_0, x = attn_output_17_cast_fp16)[name = tensor<string, []>("transpose_100")];
            tensor<fp16, [1, ?, 768]> var_379_cast_fp16 = reshape(shape = concat_20x, x = var_376_cast_fp16)[name = tensor<string, []>("op_379_cast_fp16")];
            tensor<fp16, [768, 768]> bert_encoder_layer_4_attention_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_4_attention_output_dense_weight_to_fp16"), val = tensor<fp16, [768, 768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(93496576)))];
            tensor<fp16, [768]> bert_encoder_layer_4_attention_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_4_attention_output_dense_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(94676288)))];
            tensor<fp16, [1, ?, 768]> linear_27_cast_fp16 = linear(bias = bert_encoder_layer_4_attention_output_dense_bias_to_fp16, weight = bert_encoder_layer_4_attention_output_dense_weight_to_fp16, x = var_379_cast_fp16)[name = tensor<string, []>("linear_27_cast_fp16")];
            tensor<fp16, [1, ?, 768]> input_77_cast_fp16 = add(x = linear_27_cast_fp16, y = hidden_states_25_cast_fp16)[name = tensor<string, []>("input_77_cast_fp16")];
            tensor<int32, [1]> input_79_axes_0 = const()[name = tensor<string, []>("input_79_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [768]> bert_encoder_layer_4_attention_output_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_4_attention_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(94677888)))];
            tensor<fp16, [768]> bert_encoder_layer_4_attention_output_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_4_attention_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(94679488)))];
            tensor<fp16, [1, ?, 768]> input_79_cast_fp16 = layer_norm(axes = input_79_axes_0, beta = bert_encoder_layer_4_attention_output_LayerNorm_bias_to_fp16, epsilon = var_68_to_fp16, gamma = bert_encoder_layer_4_attention_output_LayerNorm_weight_to_fp16, x = input_77_cast_fp16)[name = tensor<string, []>("input_79_cast_fp16")];
            tensor<fp16, [3072, 768]> bert_encoder_layer_4_intermediate_dense_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_4_intermediate_dense_weight_to_fp16"), val = tensor<fp16, [3072, 768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(94681088)))];
            tensor<fp16, [3072]> bert_encoder_layer_4_intermediate_dense_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_4_intermediate_dense_bias_to_fp16"), val = tensor<fp16, [3072]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(99399744)))];
            tensor<fp16, [1, ?, 3072]> linear_28_cast_fp16 = linear(bias = bert_encoder_layer_4_intermediate_dense_bias_to_fp16, weight = bert_encoder_layer_4_intermediate_dense_weight_to_fp16, x = input_79_cast_fp16)[name = tensor<string, []>("linear_28_cast_fp16")];
            tensor<string, []> input_83_mode_0 = const()[name = tensor<string, []>("input_83_mode_0"), val = tensor<string, []>("EXACT")];
            tensor<fp16, [1, ?, 3072]> input_83_cast_fp16 = gelu(mode = input_83_mode_0, x = linear_28_cast_fp16)[name = tensor<string, []>("input_83_cast_fp16")];
            tensor<fp16, [768, 3072]> bert_encoder_layer_4_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_4_output_dense_weight_to_fp16"), val = tensor<fp16, [768, 3072]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(99405952)))];
            tensor<fp16, [768]> bert_encoder_layer_4_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_4_output_dense_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(104124608)))];
            tensor<fp16, [1, ?, 768]> linear_29_cast_fp16 = linear(bias = bert_encoder_layer_4_output_dense_bias_to_fp16, weight = bert_encoder_layer_4_output_dense_weight_to_fp16, x = input_83_cast_fp16)[name = tensor<string, []>("linear_29_cast_fp16")];
            tensor<fp16, [1, ?, 768]> input_87_cast_fp16 = add(x = linear_29_cast_fp16, y = input_79_cast_fp16)[name = tensor<string, []>("input_87_cast_fp16")];
            tensor<int32, [1]> hidden_states_31_axes_0 = const()[name = tensor<string, []>("hidden_states_31_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [768]> bert_encoder_layer_4_output_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_4_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(104126208)))];
            tensor<fp16, [768]> bert_encoder_layer_4_output_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_4_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(104127808)))];
            tensor<fp16, [1, ?, 768]> hidden_states_31_cast_fp16 = layer_norm(axes = hidden_states_31_axes_0, beta = bert_encoder_layer_4_output_LayerNorm_bias_to_fp16, epsilon = var_68_to_fp16, gamma = bert_encoder_layer_4_output_LayerNorm_weight_to_fp16, x = input_87_cast_fp16)[name = tensor<string, []>("hidden_states_31_cast_fp16")];
            tensor<fp16, [768, 768]> bert_encoder_layer_5_attention_self_query_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_5_attention_self_query_weight_to_fp16"), val = tensor<fp16, [768, 768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(104129408)))];
            tensor<fp16, [768]> bert_encoder_layer_5_attention_self_query_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_5_attention_self_query_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(105309120)))];
            tensor<fp16, [1, ?, 768]> linear_30_cast_fp16 = linear(bias = bert_encoder_layer_5_attention_self_query_bias_to_fp16, weight = bert_encoder_layer_5_attention_self_query_weight_to_fp16, x = hidden_states_31_cast_fp16)[name = tensor<string, []>("linear_30_cast_fp16")];
            tensor<int32, [4]> concat_21x = const()[name = tensor<string, []>("concat_21x"), val = tensor<int32, [4]>([1, -1, 12, 64])];
            tensor<fp16, [1, ?, 12, 64]> var_422_cast_fp16 = reshape(shape = concat_21x, x = linear_30_cast_fp16)[name = tensor<string, []>("op_422_cast_fp16")];
            tensor<fp16, [768, 768]> bert_encoder_layer_5_attention_self_key_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_5_attention_self_key_weight_to_fp16"), val = tensor<fp16, [768, 768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(105310720)))];
            tensor<fp16, [768]> bert_encoder_layer_5_attention_self_key_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_5_attention_self_key_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(106490432)))];
            tensor<fp16, [1, ?, 768]> linear_31_cast_fp16 = linear(bias = bert_encoder_layer_5_attention_self_key_bias_to_fp16, weight = bert_encoder_layer_5_attention_self_key_weight_to_fp16, x = hidden_states_31_cast_fp16)[name = tensor<string, []>("linear_31_cast_fp16")];
            tensor<int32, [4]> concat_22x = const()[name = tensor<string, []>("concat_22x"), val = tensor<int32, [4]>([1, -1, 12, 64])];
            tensor<fp16, [1, ?, 12, 64]> var_428_cast_fp16 = reshape(shape = concat_22x, x = linear_31_cast_fp16)[name = tensor<string, []>("op_428_cast_fp16")];
            tensor<fp16, [768, 768]> bert_encoder_layer_5_attention_self_value_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_5_attention_self_value_weight_to_fp16"), val = tensor<fp16, [768, 768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(106492032)))];
            tensor<fp16, [768]> bert_encoder_layer_5_attention_self_value_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_5_attention_self_value_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(107671744)))];
            tensor<fp16, [1, ?, 768]> linear_32_cast_fp16 = linear(bias = bert_encoder_layer_5_attention_self_value_bias_to_fp16, weight = bert_encoder_layer_5_attention_self_value_weight_to_fp16, x = hidden_states_31_cast_fp16)[name = tensor<string, []>("linear_32_cast_fp16")];
            tensor<int32, [4]> concat_23x = const()[name = tensor<string, []>("concat_23x"), val = tensor<int32, [4]>([1, -1, 12, 64])];
            tensor<fp16, [1, ?, 12, 64]> var_434_cast_fp16 = reshape(shape = concat_23x, x = linear_32_cast_fp16)[name = tensor<string, []>("op_434_cast_fp16")];
            tensor<int32, [4]> value_11_perm_0 = const()[name = tensor<string, []>("value_11_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<fp16, [1, ?, 12, 64]> mul_5_cast_fp16 = mul(x = var_422_cast_fp16, y = var_77_to_fp16)[name = tensor<string, []>("mul_5_cast_fp16")];
            tensor<bool, []> matmul_5_transpose_y_0 = const()[name = tensor<string, []>("matmul_5_transpose_y_0"), val = tensor<bool, []>(true)];
            tensor<bool, []> matmul_5_transpose_x_0 = const()[name = tensor<string, []>("matmul_5_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<int32, [4]> transpose_58_perm_0 = const()[name = tensor<string, []>("transpose_58_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<int32, [4]> transpose_59_perm_0 = const()[name = tensor<string, []>("transpose_59_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<fp16, [1, 12, ?, 64]> transpose_59 = transpose(perm = transpose_59_perm_0, x = var_428_cast_fp16)[name = tensor<string, []>("transpose_97")];
            tensor<fp16, [1, 12, ?, 64]> transpose_58 = transpose(perm = transpose_58_perm_0, x = mul_5_cast_fp16)[name = tensor<string, []>("transpose_98")];
            tensor<fp16, [1, 12, ?, ?]> matmul_5_cast_fp16 = matmul(transpose_x = matmul_5_transpose_x_0, transpose_y = matmul_5_transpose_y_0, x = transpose_58, y = transpose_59)[name = tensor<string, []>("matmul_5_cast_fp16")];
            tensor<fp16, [1, 12, ?, ?]> add_5_cast_fp16 = add(x = matmul_5_cast_fp16, y = attention_mask_cast_fp16)[name = tensor<string, []>("add_5_cast_fp16")];
            tensor<int32, []> softmax_5_axis_0 = const()[name = tensor<string, []>("softmax_5_axis_0"), val = tensor<int32, []>(-1)];
            tensor<fp16, [1, 12, ?, ?]> softmax_5_cast_fp16 = softmax(axis = softmax_5_axis_0, x = add_5_cast_fp16)[name = tensor<string, []>("softmax_5_cast_fp16")];
            tensor<bool, []> attn_output_21_transpose_x_0 = const()[name = tensor<string, []>("attn_output_21_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> attn_output_21_transpose_y_0 = const()[name = tensor<string, []>("attn_output_21_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 12, ?, 64]> value_11_cast_fp16 = transpose(perm = value_11_perm_0, x = var_434_cast_fp16)[name = tensor<string, []>("transpose_99")];
            tensor<fp16, [1, 12, ?, 64]> attn_output_21_cast_fp16 = matmul(transpose_x = attn_output_21_transpose_x_0, transpose_y = attn_output_21_transpose_y_0, x = softmax_5_cast_fp16, y = value_11_cast_fp16)[name = tensor<string, []>("attn_output_21_cast_fp16")];
            tensor<int32, [4]> var_437_perm_0 = const()[name = tensor<string, []>("op_437_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [3]> concat_24x = const()[name = tensor<string, []>("concat_24x"), val = tensor<int32, [3]>([1, -1, 768])];
            tensor<fp16, [1, ?, 12, 64]> var_437_cast_fp16 = transpose(perm = var_437_perm_0, x = attn_output_21_cast_fp16)[name = tensor<string, []>("transpose_96")];
            tensor<fp16, [1, ?, 768]> var_440_cast_fp16 = reshape(shape = concat_24x, x = var_437_cast_fp16)[name = tensor<string, []>("op_440_cast_fp16")];
            tensor<fp16, [768, 768]> bert_encoder_layer_5_attention_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_5_attention_output_dense_weight_to_fp16"), val = tensor<fp16, [768, 768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(107673344)))];
            tensor<fp16, [768]> bert_encoder_layer_5_attention_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_5_attention_output_dense_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(108853056)))];
            tensor<fp16, [1, ?, 768]> linear_33_cast_fp16 = linear(bias = bert_encoder_layer_5_attention_output_dense_bias_to_fp16, weight = bert_encoder_layer_5_attention_output_dense_weight_to_fp16, x = var_440_cast_fp16)[name = tensor<string, []>("linear_33_cast_fp16")];
            tensor<fp16, [1, ?, 768]> input_93_cast_fp16 = add(x = linear_33_cast_fp16, y = hidden_states_31_cast_fp16)[name = tensor<string, []>("input_93_cast_fp16")];
            tensor<int32, [1]> input_95_axes_0 = const()[name = tensor<string, []>("input_95_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [768]> bert_encoder_layer_5_attention_output_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_5_attention_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(108854656)))];
            tensor<fp16, [768]> bert_encoder_layer_5_attention_output_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_5_attention_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(108856256)))];
            tensor<fp16, [1, ?, 768]> input_95_cast_fp16 = layer_norm(axes = input_95_axes_0, beta = bert_encoder_layer_5_attention_output_LayerNorm_bias_to_fp16, epsilon = var_68_to_fp16, gamma = bert_encoder_layer_5_attention_output_LayerNorm_weight_to_fp16, x = input_93_cast_fp16)[name = tensor<string, []>("input_95_cast_fp16")];
            tensor<fp16, [3072, 768]> bert_encoder_layer_5_intermediate_dense_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_5_intermediate_dense_weight_to_fp16"), val = tensor<fp16, [3072, 768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(108857856)))];
            tensor<fp16, [3072]> bert_encoder_layer_5_intermediate_dense_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_5_intermediate_dense_bias_to_fp16"), val = tensor<fp16, [3072]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(113576512)))];
            tensor<fp16, [1, ?, 3072]> linear_34_cast_fp16 = linear(bias = bert_encoder_layer_5_intermediate_dense_bias_to_fp16, weight = bert_encoder_layer_5_intermediate_dense_weight_to_fp16, x = input_95_cast_fp16)[name = tensor<string, []>("linear_34_cast_fp16")];
            tensor<string, []> input_99_mode_0 = const()[name = tensor<string, []>("input_99_mode_0"), val = tensor<string, []>("EXACT")];
            tensor<fp16, [1, ?, 3072]> input_99_cast_fp16 = gelu(mode = input_99_mode_0, x = linear_34_cast_fp16)[name = tensor<string, []>("input_99_cast_fp16")];
            tensor<fp16, [768, 3072]> bert_encoder_layer_5_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_5_output_dense_weight_to_fp16"), val = tensor<fp16, [768, 3072]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(113582720)))];
            tensor<fp16, [768]> bert_encoder_layer_5_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_5_output_dense_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(118301376)))];
            tensor<fp16, [1, ?, 768]> linear_35_cast_fp16 = linear(bias = bert_encoder_layer_5_output_dense_bias_to_fp16, weight = bert_encoder_layer_5_output_dense_weight_to_fp16, x = input_99_cast_fp16)[name = tensor<string, []>("linear_35_cast_fp16")];
            tensor<fp16, [1, ?, 768]> input_103_cast_fp16 = add(x = linear_35_cast_fp16, y = input_95_cast_fp16)[name = tensor<string, []>("input_103_cast_fp16")];
            tensor<int32, [1]> hidden_states_37_axes_0 = const()[name = tensor<string, []>("hidden_states_37_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [768]> bert_encoder_layer_5_output_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_5_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(118302976)))];
            tensor<fp16, [768]> bert_encoder_layer_5_output_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_5_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(118304576)))];
            tensor<fp16, [1, ?, 768]> hidden_states_37_cast_fp16 = layer_norm(axes = hidden_states_37_axes_0, beta = bert_encoder_layer_5_output_LayerNorm_bias_to_fp16, epsilon = var_68_to_fp16, gamma = bert_encoder_layer_5_output_LayerNorm_weight_to_fp16, x = input_103_cast_fp16)[name = tensor<string, []>("hidden_states_37_cast_fp16")];
            tensor<fp16, [768, 768]> bert_encoder_layer_6_attention_self_query_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_6_attention_self_query_weight_to_fp16"), val = tensor<fp16, [768, 768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(118306176)))];
            tensor<fp16, [768]> bert_encoder_layer_6_attention_self_query_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_6_attention_self_query_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(119485888)))];
            tensor<fp16, [1, ?, 768]> linear_36_cast_fp16 = linear(bias = bert_encoder_layer_6_attention_self_query_bias_to_fp16, weight = bert_encoder_layer_6_attention_self_query_weight_to_fp16, x = hidden_states_37_cast_fp16)[name = tensor<string, []>("linear_36_cast_fp16")];
            tensor<int32, [4]> concat_25x = const()[name = tensor<string, []>("concat_25x"), val = tensor<int32, [4]>([1, -1, 12, 64])];
            tensor<fp16, [1, ?, 12, 64]> var_483_cast_fp16 = reshape(shape = concat_25x, x = linear_36_cast_fp16)[name = tensor<string, []>("op_483_cast_fp16")];
            tensor<fp16, [768, 768]> bert_encoder_layer_6_attention_self_key_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_6_attention_self_key_weight_to_fp16"), val = tensor<fp16, [768, 768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(119487488)))];
            tensor<fp16, [768]> bert_encoder_layer_6_attention_self_key_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_6_attention_self_key_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(120667200)))];
            tensor<fp16, [1, ?, 768]> linear_37_cast_fp16 = linear(bias = bert_encoder_layer_6_attention_self_key_bias_to_fp16, weight = bert_encoder_layer_6_attention_self_key_weight_to_fp16, x = hidden_states_37_cast_fp16)[name = tensor<string, []>("linear_37_cast_fp16")];
            tensor<int32, [4]> concat_26x = const()[name = tensor<string, []>("concat_26x"), val = tensor<int32, [4]>([1, -1, 12, 64])];
            tensor<fp16, [1, ?, 12, 64]> var_489_cast_fp16 = reshape(shape = concat_26x, x = linear_37_cast_fp16)[name = tensor<string, []>("op_489_cast_fp16")];
            tensor<fp16, [768, 768]> bert_encoder_layer_6_attention_self_value_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_6_attention_self_value_weight_to_fp16"), val = tensor<fp16, [768, 768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(120668800)))];
            tensor<fp16, [768]> bert_encoder_layer_6_attention_self_value_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_6_attention_self_value_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(121848512)))];
            tensor<fp16, [1, ?, 768]> linear_38_cast_fp16 = linear(bias = bert_encoder_layer_6_attention_self_value_bias_to_fp16, weight = bert_encoder_layer_6_attention_self_value_weight_to_fp16, x = hidden_states_37_cast_fp16)[name = tensor<string, []>("linear_38_cast_fp16")];
            tensor<int32, [4]> concat_27x = const()[name = tensor<string, []>("concat_27x"), val = tensor<int32, [4]>([1, -1, 12, 64])];
            tensor<fp16, [1, ?, 12, 64]> var_495_cast_fp16 = reshape(shape = concat_27x, x = linear_38_cast_fp16)[name = tensor<string, []>("op_495_cast_fp16")];
            tensor<int32, [4]> value_13_perm_0 = const()[name = tensor<string, []>("value_13_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<fp16, [1, ?, 12, 64]> mul_6_cast_fp16 = mul(x = var_483_cast_fp16, y = var_77_to_fp16)[name = tensor<string, []>("mul_6_cast_fp16")];
            tensor<bool, []> matmul_6_transpose_y_0 = const()[name = tensor<string, []>("matmul_6_transpose_y_0"), val = tensor<bool, []>(true)];
            tensor<bool, []> matmul_6_transpose_x_0 = const()[name = tensor<string, []>("matmul_6_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<int32, [4]> transpose_60_perm_0 = const()[name = tensor<string, []>("transpose_60_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<int32, [4]> transpose_61_perm_0 = const()[name = tensor<string, []>("transpose_61_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<fp16, [1, 12, ?, 64]> transpose_61 = transpose(perm = transpose_61_perm_0, x = var_489_cast_fp16)[name = tensor<string, []>("transpose_93")];
            tensor<fp16, [1, 12, ?, 64]> transpose_60 = transpose(perm = transpose_60_perm_0, x = mul_6_cast_fp16)[name = tensor<string, []>("transpose_94")];
            tensor<fp16, [1, 12, ?, ?]> matmul_6_cast_fp16 = matmul(transpose_x = matmul_6_transpose_x_0, transpose_y = matmul_6_transpose_y_0, x = transpose_60, y = transpose_61)[name = tensor<string, []>("matmul_6_cast_fp16")];
            tensor<fp16, [1, 12, ?, ?]> add_6_cast_fp16 = add(x = matmul_6_cast_fp16, y = attention_mask_cast_fp16)[name = tensor<string, []>("add_6_cast_fp16")];
            tensor<int32, []> softmax_6_axis_0 = const()[name = tensor<string, []>("softmax_6_axis_0"), val = tensor<int32, []>(-1)];
            tensor<fp16, [1, 12, ?, ?]> softmax_6_cast_fp16 = softmax(axis = softmax_6_axis_0, x = add_6_cast_fp16)[name = tensor<string, []>("softmax_6_cast_fp16")];
            tensor<bool, []> attn_output_25_transpose_x_0 = const()[name = tensor<string, []>("attn_output_25_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> attn_output_25_transpose_y_0 = const()[name = tensor<string, []>("attn_output_25_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 12, ?, 64]> value_13_cast_fp16 = transpose(perm = value_13_perm_0, x = var_495_cast_fp16)[name = tensor<string, []>("transpose_95")];
            tensor<fp16, [1, 12, ?, 64]> attn_output_25_cast_fp16 = matmul(transpose_x = attn_output_25_transpose_x_0, transpose_y = attn_output_25_transpose_y_0, x = softmax_6_cast_fp16, y = value_13_cast_fp16)[name = tensor<string, []>("attn_output_25_cast_fp16")];
            tensor<int32, [4]> var_498_perm_0 = const()[name = tensor<string, []>("op_498_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [3]> concat_28x = const()[name = tensor<string, []>("concat_28x"), val = tensor<int32, [3]>([1, -1, 768])];
            tensor<fp16, [1, ?, 12, 64]> var_498_cast_fp16 = transpose(perm = var_498_perm_0, x = attn_output_25_cast_fp16)[name = tensor<string, []>("transpose_92")];
            tensor<fp16, [1, ?, 768]> var_501_cast_fp16 = reshape(shape = concat_28x, x = var_498_cast_fp16)[name = tensor<string, []>("op_501_cast_fp16")];
            tensor<fp16, [768, 768]> bert_encoder_layer_6_attention_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_6_attention_output_dense_weight_to_fp16"), val = tensor<fp16, [768, 768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(121850112)))];
            tensor<fp16, [768]> bert_encoder_layer_6_attention_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_6_attention_output_dense_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(123029824)))];
            tensor<fp16, [1, ?, 768]> linear_39_cast_fp16 = linear(bias = bert_encoder_layer_6_attention_output_dense_bias_to_fp16, weight = bert_encoder_layer_6_attention_output_dense_weight_to_fp16, x = var_501_cast_fp16)[name = tensor<string, []>("linear_39_cast_fp16")];
            tensor<fp16, [1, ?, 768]> input_109_cast_fp16 = add(x = linear_39_cast_fp16, y = hidden_states_37_cast_fp16)[name = tensor<string, []>("input_109_cast_fp16")];
            tensor<int32, [1]> input_111_axes_0 = const()[name = tensor<string, []>("input_111_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [768]> bert_encoder_layer_6_attention_output_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_6_attention_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(123031424)))];
            tensor<fp16, [768]> bert_encoder_layer_6_attention_output_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_6_attention_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(123033024)))];
            tensor<fp16, [1, ?, 768]> input_111_cast_fp16 = layer_norm(axes = input_111_axes_0, beta = bert_encoder_layer_6_attention_output_LayerNorm_bias_to_fp16, epsilon = var_68_to_fp16, gamma = bert_encoder_layer_6_attention_output_LayerNorm_weight_to_fp16, x = input_109_cast_fp16)[name = tensor<string, []>("input_111_cast_fp16")];
            tensor<fp16, [3072, 768]> bert_encoder_layer_6_intermediate_dense_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_6_intermediate_dense_weight_to_fp16"), val = tensor<fp16, [3072, 768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(123034624)))];
            tensor<fp16, [3072]> bert_encoder_layer_6_intermediate_dense_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_6_intermediate_dense_bias_to_fp16"), val = tensor<fp16, [3072]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(127753280)))];
            tensor<fp16, [1, ?, 3072]> linear_40_cast_fp16 = linear(bias = bert_encoder_layer_6_intermediate_dense_bias_to_fp16, weight = bert_encoder_layer_6_intermediate_dense_weight_to_fp16, x = input_111_cast_fp16)[name = tensor<string, []>("linear_40_cast_fp16")];
            tensor<string, []> input_115_mode_0 = const()[name = tensor<string, []>("input_115_mode_0"), val = tensor<string, []>("EXACT")];
            tensor<fp16, [1, ?, 3072]> input_115_cast_fp16 = gelu(mode = input_115_mode_0, x = linear_40_cast_fp16)[name = tensor<string, []>("input_115_cast_fp16")];
            tensor<fp16, [768, 3072]> bert_encoder_layer_6_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_6_output_dense_weight_to_fp16"), val = tensor<fp16, [768, 3072]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(127759488)))];
            tensor<fp16, [768]> bert_encoder_layer_6_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_6_output_dense_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(132478144)))];
            tensor<fp16, [1, ?, 768]> linear_41_cast_fp16 = linear(bias = bert_encoder_layer_6_output_dense_bias_to_fp16, weight = bert_encoder_layer_6_output_dense_weight_to_fp16, x = input_115_cast_fp16)[name = tensor<string, []>("linear_41_cast_fp16")];
            tensor<fp16, [1, ?, 768]> input_119_cast_fp16 = add(x = linear_41_cast_fp16, y = input_111_cast_fp16)[name = tensor<string, []>("input_119_cast_fp16")];
            tensor<int32, [1]> hidden_states_43_axes_0 = const()[name = tensor<string, []>("hidden_states_43_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [768]> bert_encoder_layer_6_output_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_6_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(132479744)))];
            tensor<fp16, [768]> bert_encoder_layer_6_output_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_6_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(132481344)))];
            tensor<fp16, [1, ?, 768]> hidden_states_43_cast_fp16 = layer_norm(axes = hidden_states_43_axes_0, beta = bert_encoder_layer_6_output_LayerNorm_bias_to_fp16, epsilon = var_68_to_fp16, gamma = bert_encoder_layer_6_output_LayerNorm_weight_to_fp16, x = input_119_cast_fp16)[name = tensor<string, []>("hidden_states_43_cast_fp16")];
            tensor<fp16, [768, 768]> bert_encoder_layer_7_attention_self_query_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_7_attention_self_query_weight_to_fp16"), val = tensor<fp16, [768, 768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(132482944)))];
            tensor<fp16, [768]> bert_encoder_layer_7_attention_self_query_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_7_attention_self_query_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(133662656)))];
            tensor<fp16, [1, ?, 768]> linear_42_cast_fp16 = linear(bias = bert_encoder_layer_7_attention_self_query_bias_to_fp16, weight = bert_encoder_layer_7_attention_self_query_weight_to_fp16, x = hidden_states_43_cast_fp16)[name = tensor<string, []>("linear_42_cast_fp16")];
            tensor<int32, [4]> concat_29x = const()[name = tensor<string, []>("concat_29x"), val = tensor<int32, [4]>([1, -1, 12, 64])];
            tensor<fp16, [1, ?, 12, 64]> var_544_cast_fp16 = reshape(shape = concat_29x, x = linear_42_cast_fp16)[name = tensor<string, []>("op_544_cast_fp16")];
            tensor<fp16, [768, 768]> bert_encoder_layer_7_attention_self_key_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_7_attention_self_key_weight_to_fp16"), val = tensor<fp16, [768, 768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(133664256)))];
            tensor<fp16, [768]> bert_encoder_layer_7_attention_self_key_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_7_attention_self_key_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(134843968)))];
            tensor<fp16, [1, ?, 768]> linear_43_cast_fp16 = linear(bias = bert_encoder_layer_7_attention_self_key_bias_to_fp16, weight = bert_encoder_layer_7_attention_self_key_weight_to_fp16, x = hidden_states_43_cast_fp16)[name = tensor<string, []>("linear_43_cast_fp16")];
            tensor<int32, [4]> concat_30x = const()[name = tensor<string, []>("concat_30x"), val = tensor<int32, [4]>([1, -1, 12, 64])];
            tensor<fp16, [1, ?, 12, 64]> var_550_cast_fp16 = reshape(shape = concat_30x, x = linear_43_cast_fp16)[name = tensor<string, []>("op_550_cast_fp16")];
            tensor<fp16, [768, 768]> bert_encoder_layer_7_attention_self_value_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_7_attention_self_value_weight_to_fp16"), val = tensor<fp16, [768, 768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(134845568)))];
            tensor<fp16, [768]> bert_encoder_layer_7_attention_self_value_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_7_attention_self_value_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(136025280)))];
            tensor<fp16, [1, ?, 768]> linear_44_cast_fp16 = linear(bias = bert_encoder_layer_7_attention_self_value_bias_to_fp16, weight = bert_encoder_layer_7_attention_self_value_weight_to_fp16, x = hidden_states_43_cast_fp16)[name = tensor<string, []>("linear_44_cast_fp16")];
            tensor<int32, [4]> concat_31x = const()[name = tensor<string, []>("concat_31x"), val = tensor<int32, [4]>([1, -1, 12, 64])];
            tensor<fp16, [1, ?, 12, 64]> var_556_cast_fp16 = reshape(shape = concat_31x, x = linear_44_cast_fp16)[name = tensor<string, []>("op_556_cast_fp16")];
            tensor<int32, [4]> value_15_perm_0 = const()[name = tensor<string, []>("value_15_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<fp16, [1, ?, 12, 64]> mul_7_cast_fp16 = mul(x = var_544_cast_fp16, y = var_77_to_fp16)[name = tensor<string, []>("mul_7_cast_fp16")];
            tensor<bool, []> matmul_7_transpose_y_0 = const()[name = tensor<string, []>("matmul_7_transpose_y_0"), val = tensor<bool, []>(true)];
            tensor<bool, []> matmul_7_transpose_x_0 = const()[name = tensor<string, []>("matmul_7_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<int32, [4]> transpose_62_perm_0 = const()[name = tensor<string, []>("transpose_62_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<int32, [4]> transpose_63_perm_0 = const()[name = tensor<string, []>("transpose_63_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<fp16, [1, 12, ?, 64]> transpose_63 = transpose(perm = transpose_63_perm_0, x = var_550_cast_fp16)[name = tensor<string, []>("transpose_89")];
            tensor<fp16, [1, 12, ?, 64]> transpose_62 = transpose(perm = transpose_62_perm_0, x = mul_7_cast_fp16)[name = tensor<string, []>("transpose_90")];
            tensor<fp16, [1, 12, ?, ?]> matmul_7_cast_fp16 = matmul(transpose_x = matmul_7_transpose_x_0, transpose_y = matmul_7_transpose_y_0, x = transpose_62, y = transpose_63)[name = tensor<string, []>("matmul_7_cast_fp16")];
            tensor<fp16, [1, 12, ?, ?]> add_7_cast_fp16 = add(x = matmul_7_cast_fp16, y = attention_mask_cast_fp16)[name = tensor<string, []>("add_7_cast_fp16")];
            tensor<int32, []> softmax_7_axis_0 = const()[name = tensor<string, []>("softmax_7_axis_0"), val = tensor<int32, []>(-1)];
            tensor<fp16, [1, 12, ?, ?]> softmax_7_cast_fp16 = softmax(axis = softmax_7_axis_0, x = add_7_cast_fp16)[name = tensor<string, []>("softmax_7_cast_fp16")];
            tensor<bool, []> attn_output_29_transpose_x_0 = const()[name = tensor<string, []>("attn_output_29_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> attn_output_29_transpose_y_0 = const()[name = tensor<string, []>("attn_output_29_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 12, ?, 64]> value_15_cast_fp16 = transpose(perm = value_15_perm_0, x = var_556_cast_fp16)[name = tensor<string, []>("transpose_91")];
            tensor<fp16, [1, 12, ?, 64]> attn_output_29_cast_fp16 = matmul(transpose_x = attn_output_29_transpose_x_0, transpose_y = attn_output_29_transpose_y_0, x = softmax_7_cast_fp16, y = value_15_cast_fp16)[name = tensor<string, []>("attn_output_29_cast_fp16")];
            tensor<int32, [4]> var_559_perm_0 = const()[name = tensor<string, []>("op_559_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [3]> concat_32x = const()[name = tensor<string, []>("concat_32x"), val = tensor<int32, [3]>([1, -1, 768])];
            tensor<fp16, [1, ?, 12, 64]> var_559_cast_fp16 = transpose(perm = var_559_perm_0, x = attn_output_29_cast_fp16)[name = tensor<string, []>("transpose_88")];
            tensor<fp16, [1, ?, 768]> var_562_cast_fp16 = reshape(shape = concat_32x, x = var_559_cast_fp16)[name = tensor<string, []>("op_562_cast_fp16")];
            tensor<fp16, [768, 768]> bert_encoder_layer_7_attention_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_7_attention_output_dense_weight_to_fp16"), val = tensor<fp16, [768, 768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(136026880)))];
            tensor<fp16, [768]> bert_encoder_layer_7_attention_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_7_attention_output_dense_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(137206592)))];
            tensor<fp16, [1, ?, 768]> linear_45_cast_fp16 = linear(bias = bert_encoder_layer_7_attention_output_dense_bias_to_fp16, weight = bert_encoder_layer_7_attention_output_dense_weight_to_fp16, x = var_562_cast_fp16)[name = tensor<string, []>("linear_45_cast_fp16")];
            tensor<fp16, [1, ?, 768]> input_125_cast_fp16 = add(x = linear_45_cast_fp16, y = hidden_states_43_cast_fp16)[name = tensor<string, []>("input_125_cast_fp16")];
            tensor<int32, [1]> input_127_axes_0 = const()[name = tensor<string, []>("input_127_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [768]> bert_encoder_layer_7_attention_output_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_7_attention_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(137208192)))];
            tensor<fp16, [768]> bert_encoder_layer_7_attention_output_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_7_attention_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(137209792)))];
            tensor<fp16, [1, ?, 768]> input_127_cast_fp16 = layer_norm(axes = input_127_axes_0, beta = bert_encoder_layer_7_attention_output_LayerNorm_bias_to_fp16, epsilon = var_68_to_fp16, gamma = bert_encoder_layer_7_attention_output_LayerNorm_weight_to_fp16, x = input_125_cast_fp16)[name = tensor<string, []>("input_127_cast_fp16")];
            tensor<fp16, [3072, 768]> bert_encoder_layer_7_intermediate_dense_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_7_intermediate_dense_weight_to_fp16"), val = tensor<fp16, [3072, 768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(137211392)))];
            tensor<fp16, [3072]> bert_encoder_layer_7_intermediate_dense_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_7_intermediate_dense_bias_to_fp16"), val = tensor<fp16, [3072]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(141930048)))];
            tensor<fp16, [1, ?, 3072]> linear_46_cast_fp16 = linear(bias = bert_encoder_layer_7_intermediate_dense_bias_to_fp16, weight = bert_encoder_layer_7_intermediate_dense_weight_to_fp16, x = input_127_cast_fp16)[name = tensor<string, []>("linear_46_cast_fp16")];
            tensor<string, []> input_131_mode_0 = const()[name = tensor<string, []>("input_131_mode_0"), val = tensor<string, []>("EXACT")];
            tensor<fp16, [1, ?, 3072]> input_131_cast_fp16 = gelu(mode = input_131_mode_0, x = linear_46_cast_fp16)[name = tensor<string, []>("input_131_cast_fp16")];
            tensor<fp16, [768, 3072]> bert_encoder_layer_7_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_7_output_dense_weight_to_fp16"), val = tensor<fp16, [768, 3072]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(141936256)))];
            tensor<fp16, [768]> bert_encoder_layer_7_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_7_output_dense_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(146654912)))];
            tensor<fp16, [1, ?, 768]> linear_47_cast_fp16 = linear(bias = bert_encoder_layer_7_output_dense_bias_to_fp16, weight = bert_encoder_layer_7_output_dense_weight_to_fp16, x = input_131_cast_fp16)[name = tensor<string, []>("linear_47_cast_fp16")];
            tensor<fp16, [1, ?, 768]> input_135_cast_fp16 = add(x = linear_47_cast_fp16, y = input_127_cast_fp16)[name = tensor<string, []>("input_135_cast_fp16")];
            tensor<int32, [1]> hidden_states_49_axes_0 = const()[name = tensor<string, []>("hidden_states_49_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [768]> bert_encoder_layer_7_output_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_7_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(146656512)))];
            tensor<fp16, [768]> bert_encoder_layer_7_output_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_7_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(146658112)))];
            tensor<fp16, [1, ?, 768]> hidden_states_49_cast_fp16 = layer_norm(axes = hidden_states_49_axes_0, beta = bert_encoder_layer_7_output_LayerNorm_bias_to_fp16, epsilon = var_68_to_fp16, gamma = bert_encoder_layer_7_output_LayerNorm_weight_to_fp16, x = input_135_cast_fp16)[name = tensor<string, []>("hidden_states_49_cast_fp16")];
            tensor<fp16, [768, 768]> bert_encoder_layer_8_attention_self_query_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_8_attention_self_query_weight_to_fp16"), val = tensor<fp16, [768, 768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(146659712)))];
            tensor<fp16, [768]> bert_encoder_layer_8_attention_self_query_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_8_attention_self_query_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(147839424)))];
            tensor<fp16, [1, ?, 768]> linear_48_cast_fp16 = linear(bias = bert_encoder_layer_8_attention_self_query_bias_to_fp16, weight = bert_encoder_layer_8_attention_self_query_weight_to_fp16, x = hidden_states_49_cast_fp16)[name = tensor<string, []>("linear_48_cast_fp16")];
            tensor<int32, [4]> concat_33x = const()[name = tensor<string, []>("concat_33x"), val = tensor<int32, [4]>([1, -1, 12, 64])];
            tensor<fp16, [1, ?, 12, 64]> var_605_cast_fp16 = reshape(shape = concat_33x, x = linear_48_cast_fp16)[name = tensor<string, []>("op_605_cast_fp16")];
            tensor<fp16, [768, 768]> bert_encoder_layer_8_attention_self_key_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_8_attention_self_key_weight_to_fp16"), val = tensor<fp16, [768, 768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(147841024)))];
            tensor<fp16, [768]> bert_encoder_layer_8_attention_self_key_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_8_attention_self_key_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(149020736)))];
            tensor<fp16, [1, ?, 768]> linear_49_cast_fp16 = linear(bias = bert_encoder_layer_8_attention_self_key_bias_to_fp16, weight = bert_encoder_layer_8_attention_self_key_weight_to_fp16, x = hidden_states_49_cast_fp16)[name = tensor<string, []>("linear_49_cast_fp16")];
            tensor<int32, [4]> concat_34x = const()[name = tensor<string, []>("concat_34x"), val = tensor<int32, [4]>([1, -1, 12, 64])];
            tensor<fp16, [1, ?, 12, 64]> var_611_cast_fp16 = reshape(shape = concat_34x, x = linear_49_cast_fp16)[name = tensor<string, []>("op_611_cast_fp16")];
            tensor<fp16, [768, 768]> bert_encoder_layer_8_attention_self_value_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_8_attention_self_value_weight_to_fp16"), val = tensor<fp16, [768, 768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(149022336)))];
            tensor<fp16, [768]> bert_encoder_layer_8_attention_self_value_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_8_attention_self_value_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(150202048)))];
            tensor<fp16, [1, ?, 768]> linear_50_cast_fp16 = linear(bias = bert_encoder_layer_8_attention_self_value_bias_to_fp16, weight = bert_encoder_layer_8_attention_self_value_weight_to_fp16, x = hidden_states_49_cast_fp16)[name = tensor<string, []>("linear_50_cast_fp16")];
            tensor<int32, [4]> concat_35x = const()[name = tensor<string, []>("concat_35x"), val = tensor<int32, [4]>([1, -1, 12, 64])];
            tensor<fp16, [1, ?, 12, 64]> var_617_cast_fp16 = reshape(shape = concat_35x, x = linear_50_cast_fp16)[name = tensor<string, []>("op_617_cast_fp16")];
            tensor<int32, [4]> value_17_perm_0 = const()[name = tensor<string, []>("value_17_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<fp16, [1, ?, 12, 64]> mul_8_cast_fp16 = mul(x = var_605_cast_fp16, y = var_77_to_fp16)[name = tensor<string, []>("mul_8_cast_fp16")];
            tensor<bool, []> matmul_8_transpose_y_0 = const()[name = tensor<string, []>("matmul_8_transpose_y_0"), val = tensor<bool, []>(true)];
            tensor<bool, []> matmul_8_transpose_x_0 = const()[name = tensor<string, []>("matmul_8_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<int32, [4]> transpose_64_perm_0 = const()[name = tensor<string, []>("transpose_64_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<int32, [4]> transpose_65_perm_0 = const()[name = tensor<string, []>("transpose_65_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<fp16, [1, 12, ?, 64]> transpose_65 = transpose(perm = transpose_65_perm_0, x = var_611_cast_fp16)[name = tensor<string, []>("transpose_85")];
            tensor<fp16, [1, 12, ?, 64]> transpose_64 = transpose(perm = transpose_64_perm_0, x = mul_8_cast_fp16)[name = tensor<string, []>("transpose_86")];
            tensor<fp16, [1, 12, ?, ?]> matmul_8_cast_fp16 = matmul(transpose_x = matmul_8_transpose_x_0, transpose_y = matmul_8_transpose_y_0, x = transpose_64, y = transpose_65)[name = tensor<string, []>("matmul_8_cast_fp16")];
            tensor<fp16, [1, 12, ?, ?]> add_8_cast_fp16 = add(x = matmul_8_cast_fp16, y = attention_mask_cast_fp16)[name = tensor<string, []>("add_8_cast_fp16")];
            tensor<int32, []> softmax_8_axis_0 = const()[name = tensor<string, []>("softmax_8_axis_0"), val = tensor<int32, []>(-1)];
            tensor<fp16, [1, 12, ?, ?]> softmax_8_cast_fp16 = softmax(axis = softmax_8_axis_0, x = add_8_cast_fp16)[name = tensor<string, []>("softmax_8_cast_fp16")];
            tensor<bool, []> attn_output_33_transpose_x_0 = const()[name = tensor<string, []>("attn_output_33_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> attn_output_33_transpose_y_0 = const()[name = tensor<string, []>("attn_output_33_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 12, ?, 64]> value_17_cast_fp16 = transpose(perm = value_17_perm_0, x = var_617_cast_fp16)[name = tensor<string, []>("transpose_87")];
            tensor<fp16, [1, 12, ?, 64]> attn_output_33_cast_fp16 = matmul(transpose_x = attn_output_33_transpose_x_0, transpose_y = attn_output_33_transpose_y_0, x = softmax_8_cast_fp16, y = value_17_cast_fp16)[name = tensor<string, []>("attn_output_33_cast_fp16")];
            tensor<int32, [4]> var_620_perm_0 = const()[name = tensor<string, []>("op_620_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [3]> concat_36x = const()[name = tensor<string, []>("concat_36x"), val = tensor<int32, [3]>([1, -1, 768])];
            tensor<fp16, [1, ?, 12, 64]> var_620_cast_fp16 = transpose(perm = var_620_perm_0, x = attn_output_33_cast_fp16)[name = tensor<string, []>("transpose_84")];
            tensor<fp16, [1, ?, 768]> var_623_cast_fp16 = reshape(shape = concat_36x, x = var_620_cast_fp16)[name = tensor<string, []>("op_623_cast_fp16")];
            tensor<fp16, [768, 768]> bert_encoder_layer_8_attention_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_8_attention_output_dense_weight_to_fp16"), val = tensor<fp16, [768, 768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(150203648)))];
            tensor<fp16, [768]> bert_encoder_layer_8_attention_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_8_attention_output_dense_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(151383360)))];
            tensor<fp16, [1, ?, 768]> linear_51_cast_fp16 = linear(bias = bert_encoder_layer_8_attention_output_dense_bias_to_fp16, weight = bert_encoder_layer_8_attention_output_dense_weight_to_fp16, x = var_623_cast_fp16)[name = tensor<string, []>("linear_51_cast_fp16")];
            tensor<fp16, [1, ?, 768]> input_141_cast_fp16 = add(x = linear_51_cast_fp16, y = hidden_states_49_cast_fp16)[name = tensor<string, []>("input_141_cast_fp16")];
            tensor<int32, [1]> input_143_axes_0 = const()[name = tensor<string, []>("input_143_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [768]> bert_encoder_layer_8_attention_output_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_8_attention_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(151384960)))];
            tensor<fp16, [768]> bert_encoder_layer_8_attention_output_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_8_attention_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(151386560)))];
            tensor<fp16, [1, ?, 768]> input_143_cast_fp16 = layer_norm(axes = input_143_axes_0, beta = bert_encoder_layer_8_attention_output_LayerNorm_bias_to_fp16, epsilon = var_68_to_fp16, gamma = bert_encoder_layer_8_attention_output_LayerNorm_weight_to_fp16, x = input_141_cast_fp16)[name = tensor<string, []>("input_143_cast_fp16")];
            tensor<fp16, [3072, 768]> bert_encoder_layer_8_intermediate_dense_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_8_intermediate_dense_weight_to_fp16"), val = tensor<fp16, [3072, 768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(151388160)))];
            tensor<fp16, [3072]> bert_encoder_layer_8_intermediate_dense_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_8_intermediate_dense_bias_to_fp16"), val = tensor<fp16, [3072]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(156106816)))];
            tensor<fp16, [1, ?, 3072]> linear_52_cast_fp16 = linear(bias = bert_encoder_layer_8_intermediate_dense_bias_to_fp16, weight = bert_encoder_layer_8_intermediate_dense_weight_to_fp16, x = input_143_cast_fp16)[name = tensor<string, []>("linear_52_cast_fp16")];
            tensor<string, []> input_147_mode_0 = const()[name = tensor<string, []>("input_147_mode_0"), val = tensor<string, []>("EXACT")];
            tensor<fp16, [1, ?, 3072]> input_147_cast_fp16 = gelu(mode = input_147_mode_0, x = linear_52_cast_fp16)[name = tensor<string, []>("input_147_cast_fp16")];
            tensor<fp16, [768, 3072]> bert_encoder_layer_8_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_8_output_dense_weight_to_fp16"), val = tensor<fp16, [768, 3072]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(156113024)))];
            tensor<fp16, [768]> bert_encoder_layer_8_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_8_output_dense_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(160831680)))];
            tensor<fp16, [1, ?, 768]> linear_53_cast_fp16 = linear(bias = bert_encoder_layer_8_output_dense_bias_to_fp16, weight = bert_encoder_layer_8_output_dense_weight_to_fp16, x = input_147_cast_fp16)[name = tensor<string, []>("linear_53_cast_fp16")];
            tensor<fp16, [1, ?, 768]> input_151_cast_fp16 = add(x = linear_53_cast_fp16, y = input_143_cast_fp16)[name = tensor<string, []>("input_151_cast_fp16")];
            tensor<int32, [1]> hidden_states_55_axes_0 = const()[name = tensor<string, []>("hidden_states_55_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [768]> bert_encoder_layer_8_output_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_8_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(160833280)))];
            tensor<fp16, [768]> bert_encoder_layer_8_output_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_8_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(160834880)))];
            tensor<fp16, [1, ?, 768]> hidden_states_55_cast_fp16 = layer_norm(axes = hidden_states_55_axes_0, beta = bert_encoder_layer_8_output_LayerNorm_bias_to_fp16, epsilon = var_68_to_fp16, gamma = bert_encoder_layer_8_output_LayerNorm_weight_to_fp16, x = input_151_cast_fp16)[name = tensor<string, []>("hidden_states_55_cast_fp16")];
            tensor<fp16, [768, 768]> bert_encoder_layer_9_attention_self_query_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_9_attention_self_query_weight_to_fp16"), val = tensor<fp16, [768, 768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(160836480)))];
            tensor<fp16, [768]> bert_encoder_layer_9_attention_self_query_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_9_attention_self_query_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(162016192)))];
            tensor<fp16, [1, ?, 768]> linear_54_cast_fp16 = linear(bias = bert_encoder_layer_9_attention_self_query_bias_to_fp16, weight = bert_encoder_layer_9_attention_self_query_weight_to_fp16, x = hidden_states_55_cast_fp16)[name = tensor<string, []>("linear_54_cast_fp16")];
            tensor<int32, [4]> concat_37x = const()[name = tensor<string, []>("concat_37x"), val = tensor<int32, [4]>([1, -1, 12, 64])];
            tensor<fp16, [1, ?, 12, 64]> var_666_cast_fp16 = reshape(shape = concat_37x, x = linear_54_cast_fp16)[name = tensor<string, []>("op_666_cast_fp16")];
            tensor<fp16, [768, 768]> bert_encoder_layer_9_attention_self_key_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_9_attention_self_key_weight_to_fp16"), val = tensor<fp16, [768, 768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(162017792)))];
            tensor<fp16, [768]> bert_encoder_layer_9_attention_self_key_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_9_attention_self_key_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(163197504)))];
            tensor<fp16, [1, ?, 768]> linear_55_cast_fp16 = linear(bias = bert_encoder_layer_9_attention_self_key_bias_to_fp16, weight = bert_encoder_layer_9_attention_self_key_weight_to_fp16, x = hidden_states_55_cast_fp16)[name = tensor<string, []>("linear_55_cast_fp16")];
            tensor<int32, [4]> concat_38x = const()[name = tensor<string, []>("concat_38x"), val = tensor<int32, [4]>([1, -1, 12, 64])];
            tensor<fp16, [1, ?, 12, 64]> var_672_cast_fp16 = reshape(shape = concat_38x, x = linear_55_cast_fp16)[name = tensor<string, []>("op_672_cast_fp16")];
            tensor<fp16, [768, 768]> bert_encoder_layer_9_attention_self_value_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_9_attention_self_value_weight_to_fp16"), val = tensor<fp16, [768, 768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(163199104)))];
            tensor<fp16, [768]> bert_encoder_layer_9_attention_self_value_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_9_attention_self_value_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(164378816)))];
            tensor<fp16, [1, ?, 768]> linear_56_cast_fp16 = linear(bias = bert_encoder_layer_9_attention_self_value_bias_to_fp16, weight = bert_encoder_layer_9_attention_self_value_weight_to_fp16, x = hidden_states_55_cast_fp16)[name = tensor<string, []>("linear_56_cast_fp16")];
            tensor<int32, [4]> concat_39x = const()[name = tensor<string, []>("concat_39x"), val = tensor<int32, [4]>([1, -1, 12, 64])];
            tensor<fp16, [1, ?, 12, 64]> var_678_cast_fp16 = reshape(shape = concat_39x, x = linear_56_cast_fp16)[name = tensor<string, []>("op_678_cast_fp16")];
            tensor<int32, [4]> value_19_perm_0 = const()[name = tensor<string, []>("value_19_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<fp16, [1, ?, 12, 64]> mul_9_cast_fp16 = mul(x = var_666_cast_fp16, y = var_77_to_fp16)[name = tensor<string, []>("mul_9_cast_fp16")];
            tensor<bool, []> matmul_9_transpose_y_0 = const()[name = tensor<string, []>("matmul_9_transpose_y_0"), val = tensor<bool, []>(true)];
            tensor<bool, []> matmul_9_transpose_x_0 = const()[name = tensor<string, []>("matmul_9_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<int32, [4]> transpose_66_perm_0 = const()[name = tensor<string, []>("transpose_66_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<int32, [4]> transpose_67_perm_0 = const()[name = tensor<string, []>("transpose_67_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<fp16, [1, 12, ?, 64]> transpose_67 = transpose(perm = transpose_67_perm_0, x = var_672_cast_fp16)[name = tensor<string, []>("transpose_81")];
            tensor<fp16, [1, 12, ?, 64]> transpose_66 = transpose(perm = transpose_66_perm_0, x = mul_9_cast_fp16)[name = tensor<string, []>("transpose_82")];
            tensor<fp16, [1, 12, ?, ?]> matmul_9_cast_fp16 = matmul(transpose_x = matmul_9_transpose_x_0, transpose_y = matmul_9_transpose_y_0, x = transpose_66, y = transpose_67)[name = tensor<string, []>("matmul_9_cast_fp16")];
            tensor<fp16, [1, 12, ?, ?]> add_9_cast_fp16 = add(x = matmul_9_cast_fp16, y = attention_mask_cast_fp16)[name = tensor<string, []>("add_9_cast_fp16")];
            tensor<int32, []> softmax_9_axis_0 = const()[name = tensor<string, []>("softmax_9_axis_0"), val = tensor<int32, []>(-1)];
            tensor<fp16, [1, 12, ?, ?]> softmax_9_cast_fp16 = softmax(axis = softmax_9_axis_0, x = add_9_cast_fp16)[name = tensor<string, []>("softmax_9_cast_fp16")];
            tensor<bool, []> attn_output_37_transpose_x_0 = const()[name = tensor<string, []>("attn_output_37_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> attn_output_37_transpose_y_0 = const()[name = tensor<string, []>("attn_output_37_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 12, ?, 64]> value_19_cast_fp16 = transpose(perm = value_19_perm_0, x = var_678_cast_fp16)[name = tensor<string, []>("transpose_83")];
            tensor<fp16, [1, 12, ?, 64]> attn_output_37_cast_fp16 = matmul(transpose_x = attn_output_37_transpose_x_0, transpose_y = attn_output_37_transpose_y_0, x = softmax_9_cast_fp16, y = value_19_cast_fp16)[name = tensor<string, []>("attn_output_37_cast_fp16")];
            tensor<int32, [4]> var_681_perm_0 = const()[name = tensor<string, []>("op_681_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [3]> concat_40x = const()[name = tensor<string, []>("concat_40x"), val = tensor<int32, [3]>([1, -1, 768])];
            tensor<fp16, [1, ?, 12, 64]> var_681_cast_fp16 = transpose(perm = var_681_perm_0, x = attn_output_37_cast_fp16)[name = tensor<string, []>("transpose_80")];
            tensor<fp16, [1, ?, 768]> var_684_cast_fp16 = reshape(shape = concat_40x, x = var_681_cast_fp16)[name = tensor<string, []>("op_684_cast_fp16")];
            tensor<fp16, [768, 768]> bert_encoder_layer_9_attention_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_9_attention_output_dense_weight_to_fp16"), val = tensor<fp16, [768, 768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(164380416)))];
            tensor<fp16, [768]> bert_encoder_layer_9_attention_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_9_attention_output_dense_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(165560128)))];
            tensor<fp16, [1, ?, 768]> linear_57_cast_fp16 = linear(bias = bert_encoder_layer_9_attention_output_dense_bias_to_fp16, weight = bert_encoder_layer_9_attention_output_dense_weight_to_fp16, x = var_684_cast_fp16)[name = tensor<string, []>("linear_57_cast_fp16")];
            tensor<fp16, [1, ?, 768]> input_157_cast_fp16 = add(x = linear_57_cast_fp16, y = hidden_states_55_cast_fp16)[name = tensor<string, []>("input_157_cast_fp16")];
            tensor<int32, [1]> input_159_axes_0 = const()[name = tensor<string, []>("input_159_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [768]> bert_encoder_layer_9_attention_output_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_9_attention_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(165561728)))];
            tensor<fp16, [768]> bert_encoder_layer_9_attention_output_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_9_attention_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(165563328)))];
            tensor<fp16, [1, ?, 768]> input_159_cast_fp16 = layer_norm(axes = input_159_axes_0, beta = bert_encoder_layer_9_attention_output_LayerNorm_bias_to_fp16, epsilon = var_68_to_fp16, gamma = bert_encoder_layer_9_attention_output_LayerNorm_weight_to_fp16, x = input_157_cast_fp16)[name = tensor<string, []>("input_159_cast_fp16")];
            tensor<fp16, [3072, 768]> bert_encoder_layer_9_intermediate_dense_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_9_intermediate_dense_weight_to_fp16"), val = tensor<fp16, [3072, 768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(165564928)))];
            tensor<fp16, [3072]> bert_encoder_layer_9_intermediate_dense_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_9_intermediate_dense_bias_to_fp16"), val = tensor<fp16, [3072]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(170283584)))];
            tensor<fp16, [1, ?, 3072]> linear_58_cast_fp16 = linear(bias = bert_encoder_layer_9_intermediate_dense_bias_to_fp16, weight = bert_encoder_layer_9_intermediate_dense_weight_to_fp16, x = input_159_cast_fp16)[name = tensor<string, []>("linear_58_cast_fp16")];
            tensor<string, []> input_163_mode_0 = const()[name = tensor<string, []>("input_163_mode_0"), val = tensor<string, []>("EXACT")];
            tensor<fp16, [1, ?, 3072]> input_163_cast_fp16 = gelu(mode = input_163_mode_0, x = linear_58_cast_fp16)[name = tensor<string, []>("input_163_cast_fp16")];
            tensor<fp16, [768, 3072]> bert_encoder_layer_9_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_9_output_dense_weight_to_fp16"), val = tensor<fp16, [768, 3072]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(170289792)))];
            tensor<fp16, [768]> bert_encoder_layer_9_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_9_output_dense_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(175008448)))];
            tensor<fp16, [1, ?, 768]> linear_59_cast_fp16 = linear(bias = bert_encoder_layer_9_output_dense_bias_to_fp16, weight = bert_encoder_layer_9_output_dense_weight_to_fp16, x = input_163_cast_fp16)[name = tensor<string, []>("linear_59_cast_fp16")];
            tensor<fp16, [1, ?, 768]> input_167_cast_fp16 = add(x = linear_59_cast_fp16, y = input_159_cast_fp16)[name = tensor<string, []>("input_167_cast_fp16")];
            tensor<int32, [1]> hidden_states_61_axes_0 = const()[name = tensor<string, []>("hidden_states_61_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [768]> bert_encoder_layer_9_output_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_9_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(175010048)))];
            tensor<fp16, [768]> bert_encoder_layer_9_output_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_9_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(175011648)))];
            tensor<fp16, [1, ?, 768]> hidden_states_61_cast_fp16 = layer_norm(axes = hidden_states_61_axes_0, beta = bert_encoder_layer_9_output_LayerNorm_bias_to_fp16, epsilon = var_68_to_fp16, gamma = bert_encoder_layer_9_output_LayerNorm_weight_to_fp16, x = input_167_cast_fp16)[name = tensor<string, []>("hidden_states_61_cast_fp16")];
            tensor<fp16, [768, 768]> bert_encoder_layer_10_attention_self_query_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_10_attention_self_query_weight_to_fp16"), val = tensor<fp16, [768, 768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(175013248)))];
            tensor<fp16, [768]> bert_encoder_layer_10_attention_self_query_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_10_attention_self_query_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(176192960)))];
            tensor<fp16, [1, ?, 768]> linear_60_cast_fp16 = linear(bias = bert_encoder_layer_10_attention_self_query_bias_to_fp16, weight = bert_encoder_layer_10_attention_self_query_weight_to_fp16, x = hidden_states_61_cast_fp16)[name = tensor<string, []>("linear_60_cast_fp16")];
            tensor<int32, [4]> concat_41x = const()[name = tensor<string, []>("concat_41x"), val = tensor<int32, [4]>([1, -1, 12, 64])];
            tensor<fp16, [1, ?, 12, 64]> var_727_cast_fp16 = reshape(shape = concat_41x, x = linear_60_cast_fp16)[name = tensor<string, []>("op_727_cast_fp16")];
            tensor<fp16, [768, 768]> bert_encoder_layer_10_attention_self_key_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_10_attention_self_key_weight_to_fp16"), val = tensor<fp16, [768, 768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(176194560)))];
            tensor<fp16, [768]> bert_encoder_layer_10_attention_self_key_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_10_attention_self_key_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(177374272)))];
            tensor<fp16, [1, ?, 768]> linear_61_cast_fp16 = linear(bias = bert_encoder_layer_10_attention_self_key_bias_to_fp16, weight = bert_encoder_layer_10_attention_self_key_weight_to_fp16, x = hidden_states_61_cast_fp16)[name = tensor<string, []>("linear_61_cast_fp16")];
            tensor<int32, [4]> concat_42x = const()[name = tensor<string, []>("concat_42x"), val = tensor<int32, [4]>([1, -1, 12, 64])];
            tensor<fp16, [1, ?, 12, 64]> var_733_cast_fp16 = reshape(shape = concat_42x, x = linear_61_cast_fp16)[name = tensor<string, []>("op_733_cast_fp16")];
            tensor<fp16, [768, 768]> bert_encoder_layer_10_attention_self_value_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_10_attention_self_value_weight_to_fp16"), val = tensor<fp16, [768, 768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(177375872)))];
            tensor<fp16, [768]> bert_encoder_layer_10_attention_self_value_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_10_attention_self_value_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(178555584)))];
            tensor<fp16, [1, ?, 768]> linear_62_cast_fp16 = linear(bias = bert_encoder_layer_10_attention_self_value_bias_to_fp16, weight = bert_encoder_layer_10_attention_self_value_weight_to_fp16, x = hidden_states_61_cast_fp16)[name = tensor<string, []>("linear_62_cast_fp16")];
            tensor<int32, [4]> concat_43x = const()[name = tensor<string, []>("concat_43x"), val = tensor<int32, [4]>([1, -1, 12, 64])];
            tensor<fp16, [1, ?, 12, 64]> var_739_cast_fp16 = reshape(shape = concat_43x, x = linear_62_cast_fp16)[name = tensor<string, []>("op_739_cast_fp16")];
            tensor<int32, [4]> value_21_perm_0 = const()[name = tensor<string, []>("value_21_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<fp16, [1, ?, 12, 64]> mul_10_cast_fp16 = mul(x = var_727_cast_fp16, y = var_77_to_fp16)[name = tensor<string, []>("mul_10_cast_fp16")];
            tensor<bool, []> matmul_10_transpose_y_0 = const()[name = tensor<string, []>("matmul_10_transpose_y_0"), val = tensor<bool, []>(true)];
            tensor<bool, []> matmul_10_transpose_x_0 = const()[name = tensor<string, []>("matmul_10_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<int32, [4]> transpose_68_perm_0 = const()[name = tensor<string, []>("transpose_68_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<int32, [4]> transpose_69_perm_0 = const()[name = tensor<string, []>("transpose_69_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<fp16, [1, 12, ?, 64]> transpose_69 = transpose(perm = transpose_69_perm_0, x = var_733_cast_fp16)[name = tensor<string, []>("transpose_77")];
            tensor<fp16, [1, 12, ?, 64]> transpose_68 = transpose(perm = transpose_68_perm_0, x = mul_10_cast_fp16)[name = tensor<string, []>("transpose_78")];
            tensor<fp16, [1, 12, ?, ?]> matmul_10_cast_fp16 = matmul(transpose_x = matmul_10_transpose_x_0, transpose_y = matmul_10_transpose_y_0, x = transpose_68, y = transpose_69)[name = tensor<string, []>("matmul_10_cast_fp16")];
            tensor<fp16, [1, 12, ?, ?]> add_10_cast_fp16 = add(x = matmul_10_cast_fp16, y = attention_mask_cast_fp16)[name = tensor<string, []>("add_10_cast_fp16")];
            tensor<int32, []> softmax_10_axis_0 = const()[name = tensor<string, []>("softmax_10_axis_0"), val = tensor<int32, []>(-1)];
            tensor<fp16, [1, 12, ?, ?]> softmax_10_cast_fp16 = softmax(axis = softmax_10_axis_0, x = add_10_cast_fp16)[name = tensor<string, []>("softmax_10_cast_fp16")];
            tensor<bool, []> attn_output_41_transpose_x_0 = const()[name = tensor<string, []>("attn_output_41_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> attn_output_41_transpose_y_0 = const()[name = tensor<string, []>("attn_output_41_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 12, ?, 64]> value_21_cast_fp16 = transpose(perm = value_21_perm_0, x = var_739_cast_fp16)[name = tensor<string, []>("transpose_79")];
            tensor<fp16, [1, 12, ?, 64]> attn_output_41_cast_fp16 = matmul(transpose_x = attn_output_41_transpose_x_0, transpose_y = attn_output_41_transpose_y_0, x = softmax_10_cast_fp16, y = value_21_cast_fp16)[name = tensor<string, []>("attn_output_41_cast_fp16")];
            tensor<int32, [4]> var_742_perm_0 = const()[name = tensor<string, []>("op_742_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [3]> concat_44x = const()[name = tensor<string, []>("concat_44x"), val = tensor<int32, [3]>([1, -1, 768])];
            tensor<fp16, [1, ?, 12, 64]> var_742_cast_fp16 = transpose(perm = var_742_perm_0, x = attn_output_41_cast_fp16)[name = tensor<string, []>("transpose_76")];
            tensor<fp16, [1, ?, 768]> var_745_cast_fp16 = reshape(shape = concat_44x, x = var_742_cast_fp16)[name = tensor<string, []>("op_745_cast_fp16")];
            tensor<fp16, [768, 768]> bert_encoder_layer_10_attention_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_10_attention_output_dense_weight_to_fp16"), val = tensor<fp16, [768, 768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(178557184)))];
            tensor<fp16, [768]> bert_encoder_layer_10_attention_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_10_attention_output_dense_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(179736896)))];
            tensor<fp16, [1, ?, 768]> linear_63_cast_fp16 = linear(bias = bert_encoder_layer_10_attention_output_dense_bias_to_fp16, weight = bert_encoder_layer_10_attention_output_dense_weight_to_fp16, x = var_745_cast_fp16)[name = tensor<string, []>("linear_63_cast_fp16")];
            tensor<fp16, [1, ?, 768]> input_173_cast_fp16 = add(x = linear_63_cast_fp16, y = hidden_states_61_cast_fp16)[name = tensor<string, []>("input_173_cast_fp16")];
            tensor<int32, [1]> input_175_axes_0 = const()[name = tensor<string, []>("input_175_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [768]> bert_encoder_layer_10_attention_output_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_10_attention_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(179738496)))];
            tensor<fp16, [768]> bert_encoder_layer_10_attention_output_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_10_attention_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(179740096)))];
            tensor<fp16, [1, ?, 768]> input_175_cast_fp16 = layer_norm(axes = input_175_axes_0, beta = bert_encoder_layer_10_attention_output_LayerNorm_bias_to_fp16, epsilon = var_68_to_fp16, gamma = bert_encoder_layer_10_attention_output_LayerNorm_weight_to_fp16, x = input_173_cast_fp16)[name = tensor<string, []>("input_175_cast_fp16")];
            tensor<fp16, [3072, 768]> bert_encoder_layer_10_intermediate_dense_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_10_intermediate_dense_weight_to_fp16"), val = tensor<fp16, [3072, 768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(179741696)))];
            tensor<fp16, [3072]> bert_encoder_layer_10_intermediate_dense_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_10_intermediate_dense_bias_to_fp16"), val = tensor<fp16, [3072]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(184460352)))];
            tensor<fp16, [1, ?, 3072]> linear_64_cast_fp16 = linear(bias = bert_encoder_layer_10_intermediate_dense_bias_to_fp16, weight = bert_encoder_layer_10_intermediate_dense_weight_to_fp16, x = input_175_cast_fp16)[name = tensor<string, []>("linear_64_cast_fp16")];
            tensor<string, []> input_179_mode_0 = const()[name = tensor<string, []>("input_179_mode_0"), val = tensor<string, []>("EXACT")];
            tensor<fp16, [1, ?, 3072]> input_179_cast_fp16 = gelu(mode = input_179_mode_0, x = linear_64_cast_fp16)[name = tensor<string, []>("input_179_cast_fp16")];
            tensor<fp16, [768, 3072]> bert_encoder_layer_10_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_10_output_dense_weight_to_fp16"), val = tensor<fp16, [768, 3072]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(184466560)))];
            tensor<fp16, [768]> bert_encoder_layer_10_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_10_output_dense_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(189185216)))];
            tensor<fp16, [1, ?, 768]> linear_65_cast_fp16 = linear(bias = bert_encoder_layer_10_output_dense_bias_to_fp16, weight = bert_encoder_layer_10_output_dense_weight_to_fp16, x = input_179_cast_fp16)[name = tensor<string, []>("linear_65_cast_fp16")];
            tensor<fp16, [1, ?, 768]> input_183_cast_fp16 = add(x = linear_65_cast_fp16, y = input_175_cast_fp16)[name = tensor<string, []>("input_183_cast_fp16")];
            tensor<int32, [1]> hidden_states_67_axes_0 = const()[name = tensor<string, []>("hidden_states_67_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [768]> bert_encoder_layer_10_output_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_10_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(189186816)))];
            tensor<fp16, [768]> bert_encoder_layer_10_output_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_10_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(189188416)))];
            tensor<fp16, [1, ?, 768]> hidden_states_67_cast_fp16 = layer_norm(axes = hidden_states_67_axes_0, beta = bert_encoder_layer_10_output_LayerNorm_bias_to_fp16, epsilon = var_68_to_fp16, gamma = bert_encoder_layer_10_output_LayerNorm_weight_to_fp16, x = input_183_cast_fp16)[name = tensor<string, []>("hidden_states_67_cast_fp16")];
            tensor<fp16, [768, 768]> bert_encoder_layer_11_attention_self_query_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_11_attention_self_query_weight_to_fp16"), val = tensor<fp16, [768, 768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(189190016)))];
            tensor<fp16, [768]> bert_encoder_layer_11_attention_self_query_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_11_attention_self_query_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(190369728)))];
            tensor<fp16, [1, ?, 768]> linear_66_cast_fp16 = linear(bias = bert_encoder_layer_11_attention_self_query_bias_to_fp16, weight = bert_encoder_layer_11_attention_self_query_weight_to_fp16, x = hidden_states_67_cast_fp16)[name = tensor<string, []>("linear_66_cast_fp16")];
            tensor<int32, [4]> concat_45x = const()[name = tensor<string, []>("concat_45x"), val = tensor<int32, [4]>([1, -1, 12, 64])];
            tensor<fp16, [1, ?, 12, 64]> var_788_cast_fp16 = reshape(shape = concat_45x, x = linear_66_cast_fp16)[name = tensor<string, []>("op_788_cast_fp16")];
            tensor<fp16, [768, 768]> bert_encoder_layer_11_attention_self_key_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_11_attention_self_key_weight_to_fp16"), val = tensor<fp16, [768, 768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(190371328)))];
            tensor<fp16, [768]> bert_encoder_layer_11_attention_self_key_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_11_attention_self_key_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(191551040)))];
            tensor<fp16, [1, ?, 768]> linear_67_cast_fp16 = linear(bias = bert_encoder_layer_11_attention_self_key_bias_to_fp16, weight = bert_encoder_layer_11_attention_self_key_weight_to_fp16, x = hidden_states_67_cast_fp16)[name = tensor<string, []>("linear_67_cast_fp16")];
            tensor<int32, [4]> concat_46x = const()[name = tensor<string, []>("concat_46x"), val = tensor<int32, [4]>([1, -1, 12, 64])];
            tensor<fp16, [1, ?, 12, 64]> var_794_cast_fp16 = reshape(shape = concat_46x, x = linear_67_cast_fp16)[name = tensor<string, []>("op_794_cast_fp16")];
            tensor<fp16, [768, 768]> bert_encoder_layer_11_attention_self_value_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_11_attention_self_value_weight_to_fp16"), val = tensor<fp16, [768, 768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(191552640)))];
            tensor<fp16, [768]> bert_encoder_layer_11_attention_self_value_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_11_attention_self_value_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(192732352)))];
            tensor<fp16, [1, ?, 768]> linear_68_cast_fp16 = linear(bias = bert_encoder_layer_11_attention_self_value_bias_to_fp16, weight = bert_encoder_layer_11_attention_self_value_weight_to_fp16, x = hidden_states_67_cast_fp16)[name = tensor<string, []>("linear_68_cast_fp16")];
            tensor<int32, [4]> concat_47x = const()[name = tensor<string, []>("concat_47x"), val = tensor<int32, [4]>([1, -1, 12, 64])];
            tensor<fp16, [1, ?, 12, 64]> var_800_cast_fp16 = reshape(shape = concat_47x, x = linear_68_cast_fp16)[name = tensor<string, []>("op_800_cast_fp16")];
            tensor<int32, [4]> value_23_perm_0 = const()[name = tensor<string, []>("value_23_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<fp16, [1, ?, 12, 64]> mul_11_cast_fp16 = mul(x = var_788_cast_fp16, y = var_77_to_fp16)[name = tensor<string, []>("mul_11_cast_fp16")];
            tensor<bool, []> matmul_11_transpose_y_0 = const()[name = tensor<string, []>("matmul_11_transpose_y_0"), val = tensor<bool, []>(true)];
            tensor<bool, []> matmul_11_transpose_x_0 = const()[name = tensor<string, []>("matmul_11_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<int32, [4]> transpose_70_perm_0 = const()[name = tensor<string, []>("transpose_70_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<int32, [4]> transpose_71_perm_0 = const()[name = tensor<string, []>("transpose_71_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<fp16, [1, 12, ?, 64]> transpose_71 = transpose(perm = transpose_71_perm_0, x = var_794_cast_fp16)[name = tensor<string, []>("transpose_73")];
            tensor<fp16, [1, 12, ?, 64]> transpose_70 = transpose(perm = transpose_70_perm_0, x = mul_11_cast_fp16)[name = tensor<string, []>("transpose_74")];
            tensor<fp16, [1, 12, ?, ?]> matmul_11_cast_fp16 = matmul(transpose_x = matmul_11_transpose_x_0, transpose_y = matmul_11_transpose_y_0, x = transpose_70, y = transpose_71)[name = tensor<string, []>("matmul_11_cast_fp16")];
            tensor<fp16, [1, 12, ?, ?]> add_11_cast_fp16 = add(x = matmul_11_cast_fp16, y = attention_mask_cast_fp16)[name = tensor<string, []>("add_11_cast_fp16")];
            tensor<int32, []> softmax_11_axis_0 = const()[name = tensor<string, []>("softmax_11_axis_0"), val = tensor<int32, []>(-1)];
            tensor<fp16, [1, 12, ?, ?]> softmax_11_cast_fp16 = softmax(axis = softmax_11_axis_0, x = add_11_cast_fp16)[name = tensor<string, []>("softmax_11_cast_fp16")];
            tensor<bool, []> attn_output_45_transpose_x_0 = const()[name = tensor<string, []>("attn_output_45_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> attn_output_45_transpose_y_0 = const()[name = tensor<string, []>("attn_output_45_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 12, ?, 64]> value_23_cast_fp16 = transpose(perm = value_23_perm_0, x = var_800_cast_fp16)[name = tensor<string, []>("transpose_75")];
            tensor<fp16, [1, 12, ?, 64]> attn_output_45_cast_fp16 = matmul(transpose_x = attn_output_45_transpose_x_0, transpose_y = attn_output_45_transpose_y_0, x = softmax_11_cast_fp16, y = value_23_cast_fp16)[name = tensor<string, []>("attn_output_45_cast_fp16")];
            tensor<int32, [4]> var_803_perm_0 = const()[name = tensor<string, []>("op_803_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [3]> concat_48x = const()[name = tensor<string, []>("concat_48x"), val = tensor<int32, [3]>([1, -1, 768])];
            tensor<fp16, [1, ?, 12, 64]> var_803_cast_fp16 = transpose(perm = var_803_perm_0, x = attn_output_45_cast_fp16)[name = tensor<string, []>("transpose_72")];
            tensor<fp16, [1, ?, 768]> var_806_cast_fp16 = reshape(shape = concat_48x, x = var_803_cast_fp16)[name = tensor<string, []>("op_806_cast_fp16")];
            tensor<fp16, [768, 768]> bert_encoder_layer_11_attention_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_11_attention_output_dense_weight_to_fp16"), val = tensor<fp16, [768, 768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(192733952)))];
            tensor<fp16, [768]> bert_encoder_layer_11_attention_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_11_attention_output_dense_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(193913664)))];
            tensor<fp16, [1, ?, 768]> linear_69_cast_fp16 = linear(bias = bert_encoder_layer_11_attention_output_dense_bias_to_fp16, weight = bert_encoder_layer_11_attention_output_dense_weight_to_fp16, x = var_806_cast_fp16)[name = tensor<string, []>("linear_69_cast_fp16")];
            tensor<fp16, [1, ?, 768]> input_189_cast_fp16 = add(x = linear_69_cast_fp16, y = hidden_states_67_cast_fp16)[name = tensor<string, []>("input_189_cast_fp16")];
            tensor<int32, [1]> input_191_axes_0 = const()[name = tensor<string, []>("input_191_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [768]> bert_encoder_layer_11_attention_output_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_11_attention_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(193915264)))];
            tensor<fp16, [768]> bert_encoder_layer_11_attention_output_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_11_attention_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(193916864)))];
            tensor<fp16, [1, ?, 768]> input_191_cast_fp16 = layer_norm(axes = input_191_axes_0, beta = bert_encoder_layer_11_attention_output_LayerNorm_bias_to_fp16, epsilon = var_68_to_fp16, gamma = bert_encoder_layer_11_attention_output_LayerNorm_weight_to_fp16, x = input_189_cast_fp16)[name = tensor<string, []>("input_191_cast_fp16")];
            tensor<fp16, [3072, 768]> bert_encoder_layer_11_intermediate_dense_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_11_intermediate_dense_weight_to_fp16"), val = tensor<fp16, [3072, 768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(193918464)))];
            tensor<fp16, [3072]> bert_encoder_layer_11_intermediate_dense_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_11_intermediate_dense_bias_to_fp16"), val = tensor<fp16, [3072]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(198637120)))];
            tensor<fp16, [1, ?, 3072]> linear_70_cast_fp16 = linear(bias = bert_encoder_layer_11_intermediate_dense_bias_to_fp16, weight = bert_encoder_layer_11_intermediate_dense_weight_to_fp16, x = input_191_cast_fp16)[name = tensor<string, []>("linear_70_cast_fp16")];
            tensor<string, []> input_195_mode_0 = const()[name = tensor<string, []>("input_195_mode_0"), val = tensor<string, []>("EXACT")];
            tensor<fp16, [1, ?, 3072]> input_195_cast_fp16 = gelu(mode = input_195_mode_0, x = linear_70_cast_fp16)[name = tensor<string, []>("input_195_cast_fp16")];
            tensor<fp16, [768, 3072]> bert_encoder_layer_11_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_11_output_dense_weight_to_fp16"), val = tensor<fp16, [768, 3072]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(198643328)))];
            tensor<fp16, [768]> bert_encoder_layer_11_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_11_output_dense_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(203361984)))];
            tensor<fp16, [1, ?, 768]> linear_71_cast_fp16 = linear(bias = bert_encoder_layer_11_output_dense_bias_to_fp16, weight = bert_encoder_layer_11_output_dense_weight_to_fp16, x = input_195_cast_fp16)[name = tensor<string, []>("linear_71_cast_fp16")];
            tensor<fp16, [1, ?, 768]> input_199_cast_fp16 = add(x = linear_71_cast_fp16, y = input_191_cast_fp16)[name = tensor<string, []>("input_199_cast_fp16")];
            tensor<int32, [1]> input_201_axes_0 = const()[name = tensor<string, []>("input_201_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [768]> bert_encoder_layer_11_output_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_11_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(203363584)))];
            tensor<fp16, [768]> bert_encoder_layer_11_output_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("bert_encoder_layer_11_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(203365184)))];
            tensor<fp16, [1, ?, 768]> input_201_cast_fp16 = layer_norm(axes = input_201_axes_0, beta = bert_encoder_layer_11_output_LayerNorm_bias_to_fp16, epsilon = var_68_to_fp16, gamma = bert_encoder_layer_11_output_LayerNorm_weight_to_fp16, x = input_199_cast_fp16)[name = tensor<string, []>("input_201_cast_fp16")];
            tensor<fp16, [768, 768]> cls_predictions_transform_dense_weight_to_fp16 = const()[name = tensor<string, []>("cls_predictions_transform_dense_weight_to_fp16"), val = tensor<fp16, [768, 768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(203366784)))];
            tensor<fp16, [768]> cls_predictions_transform_dense_bias_to_fp16 = const()[name = tensor<string, []>("cls_predictions_transform_dense_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(204546496)))];
            tensor<fp16, [1, ?, 768]> linear_72_cast_fp16 = linear(bias = cls_predictions_transform_dense_bias_to_fp16, weight = cls_predictions_transform_dense_weight_to_fp16, x = input_201_cast_fp16)[name = tensor<string, []>("linear_72_cast_fp16")];
            tensor<string, []> input_205_mode_0 = const()[name = tensor<string, []>("input_205_mode_0"), val = tensor<string, []>("EXACT")];
            tensor<fp16, [1, ?, 768]> input_205_cast_fp16 = gelu(mode = input_205_mode_0, x = linear_72_cast_fp16)[name = tensor<string, []>("input_205_cast_fp16")];
            tensor<int32, [1]> input_axes_0 = const()[name = tensor<string, []>("input_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [768]> cls_predictions_transform_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("cls_predictions_transform_LayerNorm_weight_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(204548096)))];
            tensor<fp16, [768]> cls_predictions_transform_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("cls_predictions_transform_LayerNorm_bias_to_fp16"), val = tensor<fp16, [768]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(204549696)))];
            tensor<fp16, []> var_836_to_fp16 = const()[name = tensor<string, []>("op_836_to_fp16"), val = tensor<fp16, []>(0x1p-24)];
            tensor<fp16, [1, ?, 768]> input_cast_fp16 = layer_norm(axes = input_axes_0, beta = cls_predictions_transform_LayerNorm_bias_to_fp16, epsilon = var_836_to_fp16, gamma = cls_predictions_transform_LayerNorm_weight_to_fp16, x = input_205_cast_fp16)[name = tensor<string, []>("input_cast_fp16")];
            tensor<fp16, [21128]> cls_predictions_bias_to_fp16 = const()[name = tensor<string, []>("cls_predictions_bias_to_fp16"), val = tensor<fp16, [21128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(204551296)))];
            tensor<fp16, [1, ?, 21128]> logits = linear(bias = cls_predictions_bias_to_fp16, weight = bert_embeddings_word_embeddings_weight_to_fp16, x = input_cast_fp16)[name = tensor<string, []>("linear_73_cast_fp16")];
        } -> (logits);
}